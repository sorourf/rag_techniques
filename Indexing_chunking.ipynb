{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Chunking Strategies for RAG\n",
    "\n",
    "1. Character-Based Chunking\n",
    "2. Recursive Character-Based Chunking \n",
    "3. Semantic Chunking\n",
    "4. Cluster-Based Semantic Chunking\n",
    "5. LLM-Based Semantic Chunking \n",
    "\n",
    "This notebook evaluates these chunking strategies and visualizes the number of chunks generated by each method to determine their impact on retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/brandonstarxel/chunking_evaluation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\sorou\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\Sorou\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Main Chunking Functions\n",
    "from chunking_evaluation.chunking import (\n",
    "    ClusterSemanticChunker,\n",
    "    LLMSemanticChunker,\n",
    "    FixedTokenChunker,\n",
    "    RecursiveTokenChunker,\n",
    "    KamradtModifiedChunker\n",
    ")\n",
    "# Additional Dependencies\n",
    "import tiktoken\n",
    "from chromadb.utils import embedding_functions\n",
    "from chunking_evaluation.utils import openai_token_count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 Characters:  [Document(metadata={'source': 'survey.pdf', 'page': 0, 'page_label': '1'}, page_content=\"A Survey on the Memory Mechanism of Large\\nLanguage Model based Agents\\nZeyu Zhang1, Xiaohe Bo1, Chen Ma1, Rui Li1, Xu Chen1, Quanyu Dai2,\\nJieming Zhu2, Zhenhua Dong2, Ji-Rong Wen1\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2Huawei Noah’s Ark Lab, China\\nzeyuzhang@ruc.edu.cn, xu.chen@ruc.edu.cn\\nAbstract\\nLarge language model (LLM) based agents have recently attracted much attention\\nfrom the research and industry communities. Compared with original LLMs, LLM-\\nbased agents are featured in their self-evolving capability, which is the basis for\\nsolving real-world problems that need long-term and complex agent-environment\\ninteractions. The key component to support agent-environment interactions is the\\nmemory of the agents. While previous studies have proposed many promising mem-\\nory mechanisms, they are scattered in different papers, and there lacks a systemati-\\ncal review to summarize and compare these works from a holistic perspective, fail-\\ning to abstract common and effective designing patterns for inspiring future studies.\\nTo bridge this gap, in this paper, we propose a comprehensive survey on the memory\\nmechanism of LLM-based agents. In specific, we first discuss “what is” and “why\\ndo we need” the memory in LLM-based agents. Then, we systematically review\\nprevious studies on how to design and evaluate the memory module. In addition, we\\nalso present many agent applications, where the memory module plays an important\\nrole. At last, we analyze the limitations of existing work and show important future\\ndirections. To keep up with the latest advances in this field, we create a repository\\nat https://github.com/nuster1128/LLM_Agent_Memory_Survey.\\nPersonal Assistant\\nPlease help me to explain \\n“LLM-based agent”.\\nA LLM-based agent is a \\ntype of artificial ……\\nIn which scenarios does it \\nhave applications?\\nPersonal assistant, game,\\ncode generation, ……\\n(Knowledge) According to the previous works, large \\nlanguage model based agents refer to artificial ……\\n(Context) The current topic is LLM-based agent. “It” \\nrefers to LLM-based agents in this conversation.\\nSocial Simulation\\n I'm a compassionate physician \\nspecializing in cardiology, \\ncommitted to improving patients' \\nheart health and well-being.\\nI'm a skilled nurse dedicated to \\npatient care, ensuring comfort and \\nsupporting health with empathy \\nand expertise.\\nRole-playing\\nI' m a Smurf, and \\nSmurfs are us!\\nHave you ever had \\na dream?\\nMagic is all \\naround the us!\\nJarvis, we must \\nfirst learn to run!\\nWubalubadu.\\nBdub Wuckoop.\\nI' m Batman, the \\nlights of city.\\n[Iron Man] My name is Iron Man, \\nalso known as Tony Stark. I am \\nthe founder of Stark Industries \\nand a member of the Avengers. \\nAs one of the genius inventors \\nand billionaire, I have created \\nthe most advanced armor in the \\nworld, which not only protects \\nme but also gives me incredible \\nstrength and the ability to fly.\\nOpen-world Game\\nSkills & Knowledge\\nHP\\nMP\\nSP\\nCode Generation\\ndef bubble_sort(arr):\\n    n = len(arr)\\n    # Traverse through all array elements\\n    for i in range(n):\\n        # Last i elements are already in place\\n        for j in range(0, n-i-1):\\n            # Traverse the array from 0 to n-i-1\\n            # Swap if the element found is greater \\nthan the next element\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n    return arr\\nSort the numbers in ascending order.\\nBubble sort repeatedly steps through the list, \\ncompares adjacent elements and swaps them \\nif they are in wrong order. The pass through \\nthe list is repeated until the list is sorted.\\nBubble sort can reorder a list of numbers.\\nDevelopment Group\\nRecommendation\\nI want to buy a dress for the \\ngraduation party.\\n(Context) She just bought a new \\nblue dress. So she may need a \\nwhite accessories to match it.\\n(Personal Preference) She often buys blue \\nclothes. She values the cost-effectiveness of \\nitems, especially on clothes. She likes small \\nthings with light colors, such as ear pendants.\\nYou may like this blue dress. It is \\nof good quality and great price.\\nWould you like to buy a \\nwaistband for your dress?\\nGreat! I like this bule one. \\nI will buy it for the party.\\nMedicine\\nDavid, a 38-year-old male with a history of allergies and sinus \\ninfections, has a family history of diabetes and hypertension. As \\na smoker of about one pack a day and an occasional drinker, \\nhis lifestyle choices may contribute to his health risks. After \\ntraveling to a tropical country where mosquito-borne illnesses \\nare prevalent, he has experienced symptoms such as mild \\nfatigue, headache, and muscle aches for the past week. \\nFor three days, I’ve had a \\nfever ranging from \\n100.5°F to 102°F, a rash \\non my limbs, joint pain \\nand swelling, especially \\nin my hands, episodes of \\ndiarrhea, abdominal pain, \\nand nausea, leading to a \\nloss of appetite.\\nThe patient‘s travel history and \\nsymptoms suggest dengue fever, \\na mosquito-transmitted illness.\\nFinance\\nYesterday, the financial market underwent significant fluctuations. \\nEquity markets initially slumped as investors responded to weak \\ncorporate earnings reports and concerns over global economic \\ngrowth prospects. Bond yields declined as investors sought safer \\nassets. However, sentiment improved in the afternoon following \\nbetter-than-expected economic data from a major economy, which \\nboosted investor confidence. As a result, stock prices recovered, \\nand the market closed with modest gains.\\nThe yield on the\\n                    10-year Treasury\\n                    note rose by 0.2\\npercentage points, reaching \\nits highest level in two years. \\nInvestors sought safer assets, \\nleading to increased demand \\nfor government bonds.\\nCorporate bonds experienced \\na decline in interest, with their \\nyields rising by 0.15 \\npercentage points.\\nLLM\\n Memory\\nAgent\\nEnvironment\\nFigure 1: The importance of the memory module in LLM-based agents.\\nPreprint. Under review.\\narXiv:2404.13501v1  [cs.AI]  21 Apr 2024\"), Document(metadata={'source': 'survey.pdf', 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 4\\n2 Related Surveys 5\\n2.1 Surveys on Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Surveys on Large Language Model-based Agents . . . . . . . . . . . . . . . . . . 7\\n3 What is the Memory of LLM-based Agent 7\\n3.1 Basic Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3.2 Narrow Definition of the Agent Memory . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 Broad Definition of the Agent Memory . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.4 Memory-assisted Agent-Environment Interaction . . . . . . . . . . . . . . . . . . 9\\n4 Why We Need the Memory in LLM-based Agent 10\\n4.1 Perspective of Cognitive Psychology . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n4.2 Perspective of Self-Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n4.3 Perspective of Agent Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n5 How to Implement the Memory of LLM-based Agent 11\\n5.1 Memory Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n5.1.1 Inside-trial Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n5.1.2 Cross-trial Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5.1.3 External Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5.2 Memory Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n5.2.1 Memory in Textual Form . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n5.2.2 Memory in Parametric Form . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n5.2.3 Advantages and Disadvantages of Textual and Parametric Memory . . . . . 17\\n5.3 Memory Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n5.3.1 Memory Writing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n5.3.2 Memory Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n5.3.3 Memory Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n6 How to Evaluate the Memory in LLM-based Agent 20\\n6.1 Direct Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n6.1.1 Subjective Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n6.1.2 Objective Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n6.2 Indirect Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n6.2.1 Conversation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n6.2.2 Multi-source Question-answering . . . . . . . . . . . . . . . . . . . . . . 22\\n6.2.3 Long-context Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2'), Document(metadata={'source': 'survey.pdf', 'page': 2, 'page_label': '3'}, page_content='6.2.4 Other Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n6.3 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n7 Memory-enhanced Agent Applications 23\\n7.1 Role-playing and Social Simulation . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n7.2 Personal Assistant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n7.3 Open-world Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n7.4 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n7.5 Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n7.6 Expert System in Specific Domains . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n7.7 Other Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n8 Limitations & Future Directions 27\\n8.1 More Advances in Parametric Memory . . . . . . . . . . . . . . . . . . . . . . . . 27\\n8.2 Memory in LLM-based Multi-agent Applications . . . . . . . . . . . . . . . . . . 27\\n8.3 Memory-based Lifelong Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n8.4 Memory in Humanoid Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n9 Conclusion 28\\n3'), Document(metadata={'source': 'survey.pdf', 'page': 3, 'page_label': '4'}, page_content='1 Introduction\\n\"Without memory, there is no culture. Without memory, there would be no civilization, no society, no future.\"\\nElie Wiesel, 1928-2016\\nRecently, large language models (LLMs) have achieved remarkable success in a large number of\\ndomains, ranging from artificial intelligence and software engineering to education and social sci-\\nence [1–3]. Original LLMs usually accomplish different tasks without interacting with environments.\\nHowever, to achieve the final goal of artificial general intelligence (AGI), intelligent machines should\\nbe able to improve themselves by autonomously exploring and learning from the real world. For\\nexample, if a trip-planning agent intends to book a ticket, it should send an order request to the ticket\\nwebsite, and observe the response before taking the next action. A personal assistant agent should\\nadjust its behaviors according to the user’s feedback, providing personalized responses to improve\\nuser’s satisfaction. To further push the boundary of LLMs towards AGI, recent years have witnessed a\\nlarge number of studies on LLM-based agents [3, 4], where the key is to equip LLMs with additional\\nmodules to enhance their self-evolving capability in real-world environments.\\nAmong all the added modules, memory is a key component that differentiates the agents from\\noriginal LLMs, making an agent truly an agent (see Figure 1). It plays an extremely important role\\nin determining how the agent accumulates knowledge, processes historical experience, retrieves\\ninformative knowledge to support its actions, and so on. Around the memory module, people have\\ndevoted much effort to designing its information sources, storage forms, and operation mechanisms.\\nFor example, Shinn et al. [5] incorporate both in-trial and cross-trial information to build the memory\\nmodule for enhancing the agent’s reasoning capability. Zhong et al. [6] store memory information in\\nthe form of natural languages, which is explainable and friendly to the users. Modarressi et al. [7]\\ndesign both memory reading and writing operations to interact with environments for task solving.\\nWhile previous studies have designed many promising memory modules, there still lacks a systemic\\nstudy to view the memory modules from a holistic perspective. To bridge this gap, in this paper,\\nwe comprehensively review previous studies to present clear taxonomies and key principles for\\ndesigning and evaluating the memory module. In specific, we discuss three key problems including:\\n(1) what is the memory of LLM-based agents? (2) why do we need the memory in LLM-based\\nagents? and (3) how to implement and evaluate the memory in LLM-based agents? To begin with,\\nwe detail the concepts of memory in LLM-based agents, providing both narrow and broad definitions.\\nThen, we analyze the necessity of memory in LLM-based agents, showing its importance from\\nthree perspectives including cognitive psychology, self-evolution, and agent applications. Based on\\nthe problems of “what” and “why”, we present commonly used strategies to design and evaluate\\nthe memory modules. For the memory design, we discuss previous works from three dimensions,\\nthat is, memory sources, memory forms, and memory operations. For the memory evaluation, we\\nintroduce two widely used approaches including direct evaluation and indirect evaluation via specific\\nagent tasks. Next, we discuss agent applications including role-playing, social simulation, personal\\nassistant, open-world games, code generation, recommendation, and expert systems, in order to show\\nthe importance of the memory module in practical scenarios. At last, we analyze the limitations of\\nexisting work and highlight significant future directions.\\nThe main contributions of this paper can be summarized as follows: (1) We formally define the mem-\\nory module and comprehensively analyze its necessity for LLM-based agents. (2) We systematically\\nsummarize existing studies on designing and evaluating the memory module in LLM-based agents,\\nproviding clear taxonomies and intuitive insights. (3) We present typical agent applications to show\\nthe importance of the memory module in different scenarios. (4) We analyze the key limitations of\\nexisting memory modules and show potential solutions for inspiring future studies. To our knowledge,\\nthis is the first survey on the memory mechanism of LLM-based agents.\\nThe rest of this survey is organized as follows. First, we provide a systematical meta-survey for the\\nfields of LLMs and LLM-based agents in Section 2, categorizing different surveys and summarizing\\ntheir key contributions. Then, we discuss the problems of “what is”, “why do we need” and “how\\nto implement and evaluate” the memory module in LLM-based agents in Section 3 to 6. Next, we\\nshow the applications of memory-enhanced agents in Section 7. The discussions of the limitations of\\nexisting work and future directions come at last in Section 8 and Section 9.\\n4'), Document(metadata={'source': 'survey.pdf', 'page': 4, 'page_label': '5'}, page_content='2 Related Surveys\\nIn the past two years, LLMs have attracted much attention from the academic and industry commu-\\nnities. To systemically summarize the studies in this field, researchers have written a lot of survey\\npapers. In this section, we briefly review these surveys (see Figure 2 for an overview), highlighting\\ntheir major focuses and contributions to better position our study.\\n2.1 Surveys on Large Language Models\\nIn the field of LLMs, Zhao et al. [70] present the first comprehensive survey to summarize the\\nbackground, evolution paths, model architectures, training methodologies, and evaluation strategies\\nof LLMs. Hadi et al. [71] and Min et al. [72] also conduct LLM surveys from the holistic view,\\nwhich, however, provide different taxonomies and understandings on LLMs. Following these surveys,\\npeople dive into specific aspects of LLMs and review the corresponding milestone studies and key\\ntechnologies. These aspects can be classified into four categories including the fundamental problems,\\nevaluation, applications, and challenges of LLMs.\\nFundamental problems. The surveys in this category aim to summarize techniques that can\\nbe leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\\ncomprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\\ntraining LLMs. Shen et al. [9], Wang et al.[10] and Liu et al. [11] present surveys on the alignment of\\nLLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\\net al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\\nis key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\\net al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\\nis fundamental for LLMs to expand their capability in domains that require specialized knowledge.\\nWang et al. [13], Yao et al. [14], Wang et al. [15], Feng et al. [16] and Zhang et al. [17] present\\nsurveys on the direction of LLM knowledge editing, which is important for customizing LLMs to\\nsatisfy specific requirements. Huang et al. [19], Wang et al. [20] and Pawar et al. [21] focus on\\nlong-context capabilities of LLMs, which is critical for LLMs to process more information at each\\ntime and enhance their application scenarios. Wu et al. [22], Song et al. [23], Caffagni et al. [24] and\\nYin et al. [25] summarize multi-modal LLMs, which expands the capability of LLMs from text to\\nvisual and other modalities. The above surveys mainly focus on the effectiveness of LLMs. Another\\nimportant aspect of LLMs is their training and inference efficiency. To summarize studies on this\\naspect, Zhu et al. [30], Xu and McAuley [31], Wang et al. [32] and Park et al. [33] systematically\\nreview the techniques of model compression. Ding et al. [81] and Xu et al. [29] analyze and conclude\\nthe studies on parameter efficient fine-tuning. Bai et al. [26], Wan et al. [27], Miao et al. [28] and\\nDing et al. [81] put more focuses on the efficiency of resource utilization in a general sense.\\nEvaluation. The surveys in this category focus on how to evaluate the capability of LLMs. Specifi-\\ncally, Chang et al. [34] comprehensively summarize the evaluation methods from an overall perspec-\\ntive. It encompasses different evaluation tasks, methods, and benchmarks, which serve as critical\\nparts in assessing LLM performances. Guo et al. [35] care more about the evaluation targets and\\ndescribe how to evaluate the knowledge, alignment, and safety control capabilities of LLMs, which\\nsupplement evaluation metrics beyond performance.\\nApplications. The surveys in this category aim to summarize models that leverage LLMs to improve\\ndifferent applications. More concretely, Zhu et al. [37] focus on the field of information retrieval\\n(IR) and summarize studies on LLM-based query processes. Xu et al. [38] pay more attention to\\ninformation extraction (IE) and provide comprehensive taxonomies for LLM-based models in this\\nfield. Li et al. [50], Lin et al. [51] and Wang et al. [52] discuss the applications of LLMs in the field\\nof recommender system, where they utilize agents to generate data and provide recommendations.\\nFan et al. [39], Wang et al. [40], and Zheng et al. [41] concentrate on how LLMs can benefit software\\nengineering (SE) in terms of software design, development, and testing. Zeng et al. [42] summarize\\nLLM-based methods in the field of robotics. Cui et al. [43] and Yang et al. [44] focus on the\\napplication of autonomous driving and summarize models in this domain based on LLMs from\\ndifferent perspectives. Beyond the above domains in artificial intelligence, LLMs have also been\\nused in natural and social science. He et al. [45], Zhou et al. [46] and Wang et al. [47] summarize the\\napplications of LLMs in medicine. Li et al. [48] focus on the applications of LLMs in finance. He\\net al. [49] review the models on leveraging LLMs to improve the development of psychology.\\n5'), Document(metadata={'source': 'survey.pdf', 'page': 5, 'page_label': '6'}, page_content='LLMs\\n[70–73]\\nChallenges\\nSecurity\\nYao et al. [64], Shayegani et al.\\n[65], Neel and Chang [66], Smith et al.\\n[67], Dong et al. [68], Das et al. [69].\\nExplainability Zhao et al. [63].\\nBias & Fairness Gallegos et al. [60], Kotek\\net al. [61], Li et al. [62].\\nHallucination\\nZhang et al. [53], Huang et al.\\n[54], Rawte et al. [55], Ye\\net al. [56], Ji et al. [57], Ton-\\nmoy et al. [58], Jiang et al. [59].\\nApplications\\nRecommendation Li et al. [50], Lin et al.\\n[51], Wang et al. [52].\\nPsychology He et al. [49].\\nFinance Li et al. [48].\\nMedecine He et al. [45], Zhou et al.\\n[46], Wang et al. [47].\\nAutonomous Driving Cui et al. [43], Yang et al. [44].\\nRobotics Zeng et al. [42].\\nSoftware Engineering Fan et al. [39], Wang et al.\\n[40], Zheng et al. [41].\\nInformation Processing Yang et al. [36], Zhu\\net al. [37], Xu et al. [38].\\nEvaluation Chang et al. [34], Guo et al. [35].\\nFundamental Problems\\nEfficiency\\nBai et al. [26], Wan et al. [27], Miao\\net al. [28, 28], Xu et al. [29], Zhu\\net al. [30], Xu and McAuley [31],\\nWang et al. [32], Park et al. [33].\\nMultimodal Wu et al. [22], Song et al. [23],\\nCaffagni et al. [24], Yin et al. [25].\\nLong-context Huang et al. [19], Wang\\net al. [20], Pawar et al. [21].\\nTool Usage Qin et al. [18].\\nKnowledge Editing\\nWang et al. [13], Yao et al.\\n[14], Wang et al. [15], Feng\\net al. [16], Zhang et al. [17].\\nRetrieval Augmentation Gao et al. [12].\\nAlignment Shen et al. [9], Wang\\net al. [10], Liu et al. [11].\\nSupervised Fine-tuning Zhang et al. [8].\\nLLM-based Agent\\n[3, 4, 77–80]\\nChallenges\\nApplications Li et al. [76].\\nEvaluation\\nFundamental Problems\\nMulti-agents Guo et al. [75].\\nMemory Our Survey.\\nPlanning Huang et al. [74].\\nFigure 2: The organization of related surveys on LLMs and LLM-based agents.\\n6'), Document(metadata={'source': 'survey.pdf', 'page': 6, 'page_label': '7'}, page_content='Challenges. The surveys in this category focus on trustworthiness in LLMs, such as hallucination,\\nbias, unfairness, explainability, security, and privacy. Hallucination in LLMs refers to the problem\\nthat LLMs may generate misconceptions or fabrications, impacting their reliability for downstream\\napplications. Zhang et al. [53], Huang et al. [54], Rawte et al. [55], Ye et al. [56], Ji et al. [57],\\nTonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the\\nhallucination problem in LLMs. The bias and unfairness problems refer to the phenomenon that\\nLLMs may unequally treat different humans or objectives, which can lead to the propagation of\\nsocietal stereotypes and discrimination. Gallegos et al. [60], Kotek et al. [61] and Li et al. [62]\\ncomprehensively discuss these challenges and summarize existing methods for alleviating them. The\\nproblem of explainability means that the internal working mechanisms of LLMs are still unclear.\\nZhao et al. [63] systematically discuss this problem and summarize previous efforts on improving\\nthe explainability of LLMs. Security and privacy are also challenging problems, which have been\\ncomprehensively surveyed in Yao et al.[64], Shayegani et al. [65], Neel and Chang [66], Smith et al.\\n[67], Dong et al. [68] and Das et al. [69].\\n2.2 Surveys on Large Language Model-based Agents\\nBased on the capability of LLMs, people have conducted a lot of studies on building LLM-based\\nagents, which can autonomously perceive environments, take actions, accumulate knowledge, and\\nevolve themselves. In this field, Wang et al. [3] present the first survey paper to systematically\\nsummarize LLM-based agents from the perspectives of agent construction, agent application, and\\nagent evaluation. Xi et al. [4], Zhao et al. [77], Cheng et al. [78] and Ge et al. [80] also summarize\\nLLM-based agent studies from the overall perspective, but they have different focuses and taxonomies,\\ndelivering more diverse understandings on this field. In addition to these overall surveys, there have\\nalso emerged several papers reviewing specific aspects of LLM-based agents. For the fundamental\\nproblems, Durante et al. [79] summarize studies on multi-modal agents. Huang et al. [74] focus on\\nthe planning capability of LLM-based agents. Guo et al. [75] pay more attention to the scenarios of\\nmulti-agent interactions. For the applications, Li et al. [76] provide a summarization on LLM-based\\nagents that are leveraged as personal assistants.\\nPosition of this work. Our survey summarizes the studies on a fundamental problem of LLM-based\\nagents, that is, the agent’s memory mechanism. To our knowledge, this is the first survey in this\\ndirection. We hope it can not only inspire more advanced memory architectures in the future, but also\\nprovide newcomers with comprehensive starting materials.\\n3 What is the Memory of LLM-based Agent\\nInteracting and learning from environments is a basic requirement of LLM-based agents. In the\\nagent-environment interaction process, there are three key phases, that is, (1) the agent perceives\\ninformation from the environment, and stores it into the memory; (2) the agent processes the stored\\ninformation to make it more usable; and (3) the agent takes the next action based on the processed\\nmemory information. In all these phases, memory plays an extremely important role. In the following,\\nwe first define the memory of the agent from both narrow and broad perspectives, and then, detail the\\nexecution processes of the above three phases based on the memory module.\\n3.1 Basic Knowledge\\nFor clear presentations, we first introduce several important background knowledge as follows:\\nDefinition 1 (Task). Task is the final target that the agent needs to achieve, for example, booking a\\nflight ticket for Alice, recommending a restaurant for Bob, and so on. Formally, we useT to represent\\na task and label different tasks by subscripts in the following contents.\\nDefinition 2 (Environment). In a narrow sense, environment is the object that the agent needs to\\ninteract with to accomplish the task. For the examples in definition 1, the environments are Alice and\\nBob, who provide feedback on the agent’s actions. More broadly, environment can be any contextual\\nfactors that influence the agent’s decisions, such as the weather when booking flight tickets, the time\\nand location when recommending restaurants, etc.\\nDefinition 3 (Trial and Step). To accomplish a task, the agent needs to interact with the environment.\\nUsually, the agent first takes an action, and then the environment responds to this action. At last, the\\nagent takes the next action based on the response. This process iterates until the task is finished. The\\ncomplete agent-environment interaction process is called a trial, and each interaction turn is called a\\n7'), Document(metadata={'source': 'survey.pdf', 'page': 7, 'page_label': '8'}, page_content='Figure 3: (a) Examples of the potential trials in the agent-environment interaction process. (b)\\nIllustration of the memory reading, writing, and management processes, where dotted lines mean that\\nthe cross-trial information can be incorporated into the memory module.\\nstep. For each trial, the agent can take multiple steps to form a potential solution to the task. For each\\ntask, the agent can explore multiple trials to accomplish the task [5]. Formally, at step t, we use at\\nand ot to represent the agent action and the observed environment response, respectively. Then, a\\nT-length trial can be represented as ξT = {a1, o1, a2, o2, ..., oT , aT }.\\nIn the above definitions, task and environment are the most coarse-grained concepts, while step is the\\nmost fine-grained one. They together describe the complete agent-environment interaction process.\\nA Toy Example. Based on the above definitions, we show a toy example and use it throughout the\\npaper to provide more intuitions. In this example, the agent has two tasks: (A) Making a trip plan\\nfor Alice in Beijing from 5/1/2024 to 5/3/2024, where the agent has to (i) buy a round-trip flight\\nticket, (ii) determine several attractions according to Alice’s preferences, and (iii) arrange the visiting\\norder of these attractions. (B) Recommending a movie for Alice on 5/10/2024, where the agent\\nhas to (i) determine the time that Alice is free to watch movies, and (ii) select a movie according\\nto Alice’s watching preference. For each task, we present a potential trial in the following (also\\nillustrated in Figure 3(a)). For task (A), the trial is:\\n[Step 1] Agent: I need to book flight tickets from Shanghai to Beijing on 5/1/2024, and from\\nBeijing to Shanghai on 5/3/2024 through the online ticket office. Environment (Online Ticket\\nOffice): The order has been confirmed. The departure flight takes off at 9:00 AM, and your ticket\\nnumber is EK240501SX9021. The returning flight takes off at 4:30 PM, and your ticket number is\\nLG240503FN6317. Please arrive at the airport on time for your flight.\\n[Step 2] Agent: I have acquired external knowledge from the magazine Attractions in Beijing. It\\nshows that the Great Wall, the Forbidden City and the Summer Palace are famous. According to\\nAlice’s preferences, I think she prefers ancient architecture, so I determine the Forbidden City and\\nthe Summer Palace as the destinations. Environment (Alice): Well done!\\n[Step 3] Agent: Now, I need to arrange the visiting order for Alice. According to the information\\nabove, I recommend visiting the Summer Palace after she arrives in Beijing because it is suitable to\\nspend a whole afternoon there and watch the sunset. Then, she can go to the Forbidden City the next\\nday, which would give her plenty of time. Finally, she can return to Shanghai on the third day and\\ntake a rest. Environment (Alice): I love this plan. Thanks!\\n8'), Document(metadata={'source': 'survey.pdf', 'page': 8, 'page_label': '9'}, page_content='For task (B), the trial is:\\n[Step 1] Agent: According to my previous experiences of movie recommendations for Alice, she\\nprefers to start watching movies at 9:00 PM, because she has finished work by then. I should ask\\nAlice if she is available at that time. Environment (Alice): Of course, it is a nice time!\\n[Step 2] Agent: Now, I need to find some movies that Alice has marked \"want to watch\" on Netflix.\\nEnvironment (Netflix): Alice’s want to watch: Interstellar, Planet Terror and Candle in the Tomb.\\n[Step 3] Agent: Since Alice never watches horror movies at night,Interstellar would be more suitable\\nfor her. Now, I need to confirm with Alice about the movie. Environment (Alice): Great! I like it!\\n3.2 Narrow Definition of the Agent Memory\\nIn a narrow sense, the memory of the agent is only relevant to the historical information within\\nthe same trial. Formally, for a given task, the historical information of the trial before step t is\\nξt = {a1, o1, a2, o2, ..., at−1, ot−1}, and then the memory is derived based on ξt. In the above toy\\nexample, for task (A), the agent at [step 3] needs to arrange the visiting order for Alice; at this time,\\nits memory contains the information about the selected attractions and arrival time in [step 1] and\\n[step 2]. For task (B), the agent has to choose a movie for Alice at [step 3]; at this time, its memory\\ncontains the arranged time to watch films.\\n3.3 Broad Definition of the Agent Memory\\nIn a broad sense, the memory of the agent can come from much wider sources, for example,\\nthe information across different trials and the external knowledge beyond the agent-environment\\ninteractions. Formally, given a series of sequential tasks {T1, T2, ...,TK}, for task Tk, the memory\\ninformation at step t comes from three sources: (1) the historical information within the same\\ntrial, that is, ξk\\nt = {ak\\n1 , ok\\n1 , ..., ak\\nt−1, ok\\nt−1}, where we add superscript k to label the task index.\\n(2) The historical information across different trials, that is, Ξk = {ξ1, ξ2, ..., ξk−1, ξk′\\n}, where\\nξj (j ∈ {1, ..., k− 1}) represents the trials of task j1, and ξk′\\ndenotes the previously explored trials\\nfor task Tk. (3) External knowledge, which is represented by Dk\\nt . The memory of the agent is derived\\nbased on (ξk\\nt , Ξk, Dk\\nt ). In the above toy example, for task (A), if there are several failed trials,\\nthat is, the feedback from Alice is negative, then these trials can be incorporated into the agent’s\\nmemory to avoid future similar errors (corresponding to ξk′\\n). In addition, for task (B), the agent\\nmay recommend movies relevant to the attractions that Alice has visited in task (A) to capture her\\nrecent preferences (corresponding to {ξ1, ξ2, ..., ξk−1}). In the agent decision process, it has also\\nreferred to the magazine Attractions in Beijing for making trip plans, which is the external knowledge\\n(corresponding to Dk\\nt ) for the current task Tk.\\n3.4 Memory-assisted Agent-Environment Interaction\\nAs mentioned at the beginning of Section 3, there are three key phases in the agent-environment\\ninteraction process. The agent memory module implements these phases through three operations\\nincluding memory writing, memory management, and memory reading.\\nMemory Writing. This operation aims to project the raw observations into the actually stored\\nmemory contents, which are more informative [7] and concise [6]. It corresponds to the first phase of\\nthe agent-environment interaction process. Given a task Tk, if the agent takes an action ak\\nt at step t,\\nand the environment provides an observation ok\\nt , then the memory writing operation can be formally\\nrepresented as:\\nmk\\nt = W({ak\\nt , ok\\nt }),\\nwhere W is a projecting function. mk\\nt is the finally stored memory contents, which can be either\\nnatural languages or parametric representations. In the above toy example, for task (A), the agent is\\nsupposed to remember the flight arrangement and the decision of attractions after [step 2]. For task\\n(B), the agent should memorize the fact that Alice hopes to watch movies at 9:00 PM, after [step 1].\\nMemory Management. This operation aims to process the stored memory information to make\\nit more effective, for example, summarizing high-level concepts to make the agent more general-\\n1For each task, there can be multiple trials for exploring the final solution, and all of them can be incorporated\\ninto the memory.\\n9'), Document(metadata={'source': 'survey.pdf', 'page': 9, 'page_label': '10'}, page_content='izable [6], merging similar information to reduce redundancy [ 7], and forgetting unimportant or\\nirrelevant information to remove its negative influence. This operation corresponds to the second\\nphase of the agent-environment interaction process. Let Mk\\nt−1 be the memory contents for task k\\nbefore step t, and suppose mk\\nt is the stored information at step t based on the above memory writing\\noperation, then, the memory management operation can be represented by:\\nMk\\nt = P(Mk\\nt−1, mk\\nt ),\\nwhere P is a function that iteratively processes the stored memory information. For the narrow\\nmemory definition, the iteration only happens within the same trial, and the memory is emptied when\\nthe trial is ended. For the broad memory definition, the iteration happens across different trials or\\neven tasks, as well as the integrations of external knowledge. For task (B) in the above toy example,\\nthe agent can conclude that Alice enjoys watching science fiction movies in the evening, which can\\nbe used as a default rule to make recommendations for Alice in the future.\\nMemory Reading. This operation aims to obtain important information from the memory to support\\nthe next agent action. It corresponds to the third phase of the agent-environment interaction process.\\nSuppose Mk\\nt is the memory contents for task k at step t, ck\\nt is the context of the next action, then the\\nmemory reading operation can be represented by:\\nˆMk\\nt = R(Mk\\nt , ck\\nt+1),\\nwhere R is usually implemented by computing the similarity between Mk\\nt and ck\\nt+1 [82]. ˆMk\\nt is used\\nas parts of the final prompt to drive the agent’s next action. For task (B) in the above toy example,\\nwhen the agent decides on the final recommended movie in [Step 3], it should focus on the “want to\\nwatch” list in [Step 2] and select one from it.\\nBased on the above operations, we can derive a unified function for the evolving process from\\n{ak\\nt , ok\\nt } to ak\\nt+1, that is:\\nak\\nt+1 = LLM{R(P(Mk\\nt−1, W({ak\\nt , ok\\nt })), ck\\nt+1)},\\nwhere LLM is the large language model. The complete agent-environment interaction process can be\\neasily obtained by iteratively expanding this function (see Figure 3(b) for an intuitive illustration).\\nRemark. This function provides a general formulation of the agent memorizing process. Previous\\nworks may use different specifications. For example, in [5], R and P are set as identical functions,\\nand P only takes effect at the end of a trial. In Park et al. [83], R is implemented based on three\\ncriteria including similarity, time interval, and importance, and P is realized by a reflection process\\nto obtain more abstract thoughts. In this section, we focus on the overall framework of the agent’s\\nmemory operations. More detailed realizations of W, P, and R are deferred in Section 5.\\n4 Why We Need the Memory in LLM-based Agent\\nAbove, we have introduced what is the memory of LLM-based agents. Before comprehensively\\npresenting how to implement it, in this section, we briefly show why memory is necessary for building\\nLLM-based agents, where we expand our discussion from three perspectives including cognitive\\npsychology, self-evolution, and agent applications.\\n4.1 Perspective of Cognitive Psychology\\nCognitive psychology is the scientific study of human mental processes such as attention, language\\nuse, memory, perception, problem-solving, creativity, and reasoning 2. Among these processes,\\nmemory is widely recognized as an extremely important one [84]. It is fundamental for humans to\\nlearn knowledge by accumulating important information and abstracting high-level concepts [85],\\nform social norms by remembering cultural values and individual experiences [86], take reasonable\\nbehaviors by imagining the potential positive and negative consequences [87], and among others.\\nA major goal of LLM-based agents is to replace humans for accomplishing different tasks. To make\\nagents behave like humans, following human’s working mechanisms to design the agents is a natural\\nand essential choice [88]. Since memory is important for humans, designing memory modules is also\\nsignificant for the agents. In addition, cognitive psychology has been studied for a long time, so many\\neffective human memory theories and architectures have been accumulated, which can support more\\nadvanced capabilities of the agents [89].\\n2https://en.wikipedia.org/wiki/Cognitive_psychology\\n10'), Document(metadata={'source': 'survey.pdf', 'page': 10, 'page_label': '11'}, page_content='Figure 4: An overview of the sources, forms, and operations of the memory in LLM-based agents.\\n4.2 Perspective of Self-Evolution\\nTo accomplish different practical tasks, agents have to self-evolve in dynamic environments [ 90].\\nIn the agent-environment interaction process, the memory is key to the following aspects: (1)\\nExperience accumulation. An important function of the memory is to remember past error plannings,\\ninappropriate behaviors, or failed experiences, so as to make the agent more effective for handling\\nsimilar tasks in the future [91]. This is extremely important for enhancing the learning efficiency of\\nthe agent in the self-evolving process. (2) Environment exploration. To autonomously evolve in\\nthe environment, the agents have to explore different actions and learn from the feedback [92]. By\\nremembering historical information, the memory can help to better decide when and how to make\\nexplorations, for example, focusing more on previously failed trials or actions with lower exploring\\nfrequencies [93]. (3) Knowledge abstraction. Another important function of the memory is to\\nsummarize and abstract high-level information from raw observations, which is the basis for the agent\\nto be more adaptive and generalizable to unseen environments [82]. In summary, self-evolution is the\\nbasic characteristic of LLM-based agents, and memory is of key importance to self-evolution.\\n4.3 Perspective of Agent Applications\\nIn many applications, memory is an indispensable component of the agent. For example, in a conver-\\nsational agent, the memory stores information about historical conversations, which is necessary for\\nthe agent to generate the next response. Without memory, the agent does not know the context, and\\ncannot continue the conversation [94]. In a simulation agent, memory is of great importance to make\\nthe agent consistently follow the role profiles. Without memory, the agent may easily step out of the\\nrole during the simulation process [95]. Both of the above examples show that the memory is not an\\noptional component, but is necessary for the agents to accomplish given tasks.\\nIn the above three perspectives, the first one reveals that the memory builds the cognitive basis of\\nthe agent. The second and third ones show that the memory is necessary for the agent’s evolving\\nprinciples and applications, which provide insights for designing agents with memory mechanisms.\\n5 How to Implement the Memory of LLM-based Agent\\nIn this section, we discuss the implementation of the memory module from three perspectives: memory\\nsources, memory forms, and memory operations. Memory sources refer to where the memory contents\\ncome from. Memory forms focus on how to represent the memory contents. Memory operations\\naim to process the memory contents. These three perspectives provide a comprehensive review of\\nmemory implementation methods, which is helpful for future research. For better demonstration, we\\npresent an overview of implementation methods in Figure 4.\\n5.1 Memory Sources\\nIn previous works, the memory contents may come from different sources. Based on our formulation\\nin Section 3, these sources can be classified into three categories, that is, the information inside a trial,\\nthe information across different trials, and the external knowledge. The former two are dynamically\\n11'), Document(metadata={'source': 'survey.pdf', 'page': 11, 'page_label': '12'}, page_content='Table 1: Summarization of the memory sources. We use ✓ and × to label whether or not the\\ncorresponding source is adopted in the model.\\nModels Inside-trial Information Cross-trial Information External Knowledge\\nMemoryBank [6] ✓ × ×\\nRET-LLM [7] ✓ × ✓\\nChatDB [96] ✓ × ✓\\nTiM [97] ✓ × ×\\nSCM [98] ✓ × ×\\nV oyager [99] ✓ × ×\\nMemGPT [100] ✓ × ×\\nMemoChat [94] ✓ × ×\\nMPC [101] ✓ × ×\\nGenerative Agents [83] ✓ × ×\\nRecMind [102] ✓ × ✓\\nRetroformer [103] ✓ ✓ ✓\\nExpeL [82] ✓ ✓ ✓\\nSynapse [91] ✓ ✓ ×\\nGITM [93] ✓ ✓ ✓\\nReAct [104] ✓ × ✓\\nReflexion [5] ✓ ✓ ✓\\nRecAgent [95] ✓ × ×\\nCharacter-LLM [105] ✓ × ✓\\nMAC [106] ✓ × ×\\nHuatuo [107] ✓ × ✓\\nChatDev [1] ✓ × ×\\nInteRecAgent [108] ✓ × ✓\\nMetaAgents [109] ✓ × ×\\nTPTU [110, 111] ✓ × ✓\\nMetaGPT [112] ✓ ✓ ×\\nS3 [2] ✓ × ×\\nInvestLM [113] ✓ × ✓\\ngenerated in the agent-environment interaction process (e.g., task internal information), while the\\nlatter is static information outside the loop (e.g., task external information). We summarize previous\\nworks on memory sources in Table 1.\\n5.1.1 Inside-trial Information\\nIn the agent-environment interaction process, the historical steps within a trial are usually the most\\nrelevant and informative signals to support the agent’s future actions. Almost all the previous works\\nuse this information as a part of the memory sources.\\nRepresentative Studies. Generative Agents [ 83] aims to simulate human’s daily behaviors by\\nusing LLM-based agents. The memory of an agent is derived from the historical behaviors to\\nachieve a target, for example, the collection of relevant papers when researching on a specific topic.\\nMemoChat [94] aims to chat with humans, where the memory of the agent is derived based on\\nthe conversation history of a dialogue session. TiM [ 97] aims to enhance the agent’s reasoning\\ncapability by self-generating multiple thoughts after accomplishing a task, which is used as the\\nmemory to provide more generalizable information. V oyager [99] focuses on building game agents\\nbased on Minecraft, where the memory contains executable codes of preliminary and basic actions\\nto accomplish a task. It should be noted that the inside-trial information not only includes agent-\\nenvironment interactions, but also contains interaction contexts, such as time and location information.\\n12'), Document(metadata={'source': 'survey.pdf', 'page': 12, 'page_label': '13'}, page_content='Discussion. The inside-trial information is the most obvious and intuitive source that should be\\nleveraged to construct the agent’s memory since it is highly relevant to the current task that the agent\\nhas to accomplish. However, relying solely on inside-trial information may prevent the agent from\\naccumulating valuable knowledge from various tasks and learning more generalizable information.\\nThus, many studies also explore how to effectively utilize the information across different tasks to\\nbuild the memory module, which is detailed in the following sections.\\n5.1.2 Cross-trial Information\\nFor LLM-based agents, the information accumulated across multiple trials in the environment is also\\na crucial part of the memory, typically including successful and failed actions and their insights, such\\nas failure reasons, common action patterns to succeed, and so on.\\nRepresentative Studies. One of the most prominent studies is Reflexion [5], which proposes verbal\\nreinforcement learning for LLM-based agents. It derives the experiences from past trials in verbal\\nform, and applies them in subsequent trials to improve the performance of the same task. Furthermore,\\nRetroformer [103] fine-tunes the reflection model, enabling the agent to extract cross-trial information\\nfrom past trials more effectively. In Synapse [91], the agents focus on solving the computer control\\ntasks. Their memory can record cross-trial information through successful exemplars, which would\\nbe used as references on similar trials. In ExpeL [82], the agents are required to solve a collection of\\ncomplex interactive tasks within the environment. They store and organize completed trajectories,\\nand recall similar ones for the new task. In the recalled trajectories, successful cases will be compared\\nwith failed ones to identify the patterns to succeed.\\nDiscussion. According to the accumulated memory of cross-trial information, the agents are able to\\naccumulate experiences, which is important for their evolution. Based on the past experiences, the\\nagents can adjust their actions based on the overall feedback of the whole process. In contrast to the\\ninside-trial observations, which serve as short-term memory, the trial experiences can be considered\\nas long-term memory. It utilizes feedback from different trials to support a wider range of agent\\ntrials, providing more prolonged experiential support for agents. However, the limitation lies in\\nthe fact that both inside-trial and cross-trial information require the agents to personally engage in\\nagent-environment interactions, where external experiences and knowledge are not included.\\n5.1.3 External Knowledge\\nAn important characteristic of LLM-based agents is that they can be directly communicated and con-\\ntrolled in natural languages. As such, LLM-based agents can easily incorporate external knowledge\\nin textual forms (e.g., Wikipedia 3) to facilitate their decisions.\\nRepresentative Studies. In ReAct [104], the agents are required to answer questions about general\\nknowledge by multiple reasoning steps. They can utilize Wikipedia APIs to obtain external knowledge\\nif they lack information during these steps. GITM [ 93] intends to design agents in Minecraft,\\nwhich can explore in complex and sparse-reward environments. The agents draw from the online\\nMinecraft Wiki and craft recipes to provide an infinite source of knowledge for their navigation.\\nCodeAgent [114] focuses on the repo-level code generation task, which commonly requires complex\\ndependencies and extensive documentation. It designs a web search strategy for acquiring related\\nexternal knowledge. ChatDoctor [115] adapts LLM-based agents to the medical domain. It fine-tunes\\nan acquisition process to retrieve external knowledge from Wikipedia and medical databases.\\nDiscussion. The external knowledge can be obtained from both private and public sources. It provides\\nLLM-based agents with much knowledge beyond their internal environment, which might be difficult\\nor even impossible for the agent to acquire by agent-environment interactions. Moreover, most\\nexternal knowledge can be acquired by accessing the APIs of various tools dynamically in real time\\naccording to the task needs, thus mitigating the problem of outdated knowledge. Integrating external\\nknowledge into the memory of LLM-based agents significantly expands their knowledge boundaries,\\nproviding them with unlimited, up-to-date, and well-founded knowledge for decision-making.\\n5.2 Memory Forms\\nIn general, there are two forms to represent the memory contents: textual form and parametric form.\\nIn textual form, the information is explicitly retained and recalled by natural languages. In parametric\\n3https://www.wikipedia.org\\n13'), Document(metadata={'source': 'survey.pdf', 'page': 13, 'page_label': '14'}, page_content='Table 2: Summarization of the memory forms. We use ✓ and × to label whether or not the\\ncorresponding memory form is adopted in the model.\\nModels\\nTextual Form Parametric Form\\nComplete Recent Retrieved External Fine-tuning Editing\\nMemoryBank [6] × × ✓ × × ×\\nRET-LLM [7] × × ✓ × × ×\\nChatDB [96] × × ✓ × × ×\\nTiM [97] × × ✓ × × ×\\nSCM [98] × ✓ ✓ × × ×\\nV oyager [99] × × ✓ × × ×\\nMemGPT [100] × ✓ ✓ × × ×\\nMemoChat [94] × × ✓ × × ×\\nMPC [101] × × ✓ × × ×\\nGenerative Agents [83] × × ✓ × × ×\\nRecMind [102] ✓ × × × × ×\\nRetroformer [103] ✓ × × ✓ ✓ ×\\nExpeL [82] ✓ × ✓ ✓ × ×\\nSynapse [91] × × ✓ × × ×\\nGITM [93] ✓ × ✓ ✓ × ×\\nReAct [104] ✓ × × ✓ × ×\\nReflexion [5] ✓ × × ✓ × ×\\nRecAgent [95] × ✓ ✓ × × ×\\nCharacter-LLM [105] × ✓ × × ✓ ×\\nMAC [106] × × × × × ✓\\nHuatuo [107] ✓ × × × ✓ ×\\nChatDev [1] ✓ × × × × ×\\nInteRecAgent [108] × ✓ ✓ ✓ × ×\\nMetaAgents [109] × × ✓ × × ×\\nTPTU [110, 111] ✓ × × ✓ × ×\\nMetaGPT [112] ✓ × × × × ×\\nS3 [2] × × ✓ × × ×\\nInvestLM [113] ✓ × × × ✓ ×\\nform, the memory information is encoded into parameters and implicitly influences the agent’s\\nactions. We summarize previous works on memory forms with their implementations in Table 2.\\n5.2.1 Memory in Textual Form\\nTextual form is currently the mainstream method to represent the memory contents, which is featured\\nin better interpretability, easier implementation, and faster read-write efficiency. In specific, the\\ntextual form can be both non-structured representations like raw natural languages and structured\\ninformation such as tuples, databases, and so on. In general, previous studies use the textual form\\nmemory to store four types of information including (1) complete agent-environment interactions, (2)\\nrecent agent-environment interactions, (3) retrieved agent-environment interactions, and (4) external\\nknowledge. In the former three methods, the memory leverages natural languages to describe the\\ninformation within the agent-environment interaction loop. In the former three types, they record\\nthe information inside the agent-environment interaction loop, while the last type leverages natural\\nlanguages to store information outside that loop.\\nComplete Interactions. This method stores all the information of the agent-environment interaction\\nhistory based on long-context strategies [116]. For the example in Section 3.1, the memory of the\\n14'), Document(metadata={'source': 'survey.pdf', 'page': 14, 'page_label': '15'}, page_content='agent in task (A) after step 2 can be implemented by concatenating all the information before step 2,\\nand the final textual form memory is: \"Your memory is [Step 1] (Agent) ... (Online Ticket Office) ...\\n[Step 2] ... Please infer based on your memory\".\\nIn the previous work, different models store the memory information using different strategies. For\\nexample, in LongChat [116], the agents focus on understanding natural languages in long-context\\nscenarios. It fine-tunes the foundation model for better adapting to memorize complete interactions.\\nMemory Sandbox [117] intends to alleviate the impact of irrelevant memory in conversations. It\\ndesigns a transparent and interactive method to manage the memory of agents, which removes\\nirrelevant memory before concatenating them as a prompt. Moreover, some efforts are dedicated to\\nenhancing the capacity of LLMs to handle longer contexts [118, 119].\\nWhile storing all the agent-environment interactions can maintain comprehensive information, obvious\\nlimitations exist in terms of computational cost, inference time, and inference robustness. Firstly,\\nthe fast-growing long-context memory in practice results in high computational cost during LLM\\ninference, due to the quadratic growth of the time complexity of attention computation with sequence\\nlength. It thus requires much more computing resources and significantly increases inference latency,\\nwhich hinders its practical deployment. What’s more, with its fast growth, the memory length can\\neasily exceed the upper bound of the sequence length during LLM’s pretraining, which makes a\\ntruncation of memory necessary. Thus, it can lead to information loss due to the incompleteness\\nof agent memory. Last but not least, it can lead to biases and unrobustness in LLM’s inference.\\nSpecifically, a previous research [ 120] has shown that, the positions of text segments in a long\\ncontext can greatly affect their utilization, so the memory in the long-context prompt can not be\\ntreated equally and stably. All these drawbacks show the need to design extra memory modules for\\nLLM-based agents, rather than straightforwardly concatenating all the information into a prompt.\\nRecent Interactions. This method stores and maintains the most recently acquired memories using\\nnatural languages, thereby enhancing the efficiency of memory information utilization according\\nto the Principle of Locality [121]. In task (B) of the example in Section 3.1, we can just remember\\nAlice’s preferences in the recent three years, and truncate the distant part, where the recent three\\nyears can be considered as the memory window size.\\nIn previous studies, there are various strategies to store recent textual memories. For example,\\nSCM [98] proposes a flash memory based on the cache mechanism, which preserves observations from\\nthe recent t −1 time steps, aimed at enhancing the recency of information. MemGPT [100] considers\\nthe agent as an operating system, which can dynamically interact with users through a natural interface.\\nIt designs the working context to hold recent histories, as a part of virtual context management. In\\nRecAgent [95], the agents are designed to simulate user behaviors in movie recommendations.\\nIt stores some temporal information in short-term memory as an intermediate cache, which can\\nsimulate the memory mechanism of the human brain [122, 123]. These representative methods can\\ndynamically update memories based on recent interactions, and pay more attention to the recent\\ncontext that is important for the current stage.\\nCaching the memory according to recency is an effective way to enhance memory efficiency, and it\\nenables agents to focus more on the recent information. However, in long-term tasks, this method\\nfails to access key information from distant memories. It can result in the loss of potentially crucial\\ninformation that is not within the immediate cache window. In other words, emphasizing on recency\\ncan inherently neglect earlier, yet critical information, thus posing challenges in scenarios requiring a\\ncomprehensive understanding of past events.\\nRetrieved Interactions. Unlike the above method which truncates memories based on time, this\\nmethod typically selects memory contents based on their relevance, importance, and topics. It ensures\\nthe inclusion of distant but crucial memories in the decision-making process, thereby addressing the\\nlimitation of only memorizing recent information. In task (A) of the example in Section 3.1, Alice’s\\npreferences have been stored in the memory before this task. At [Step 2], the agent will retrieve\\nthe most relevant aspects of Alice’s preferences from memory based on the query keyword \"travel\",\\nobtaining Alice’s scenic spot preference for ancient architectures. In general, retrieval methods will\\ngenerate embeddings as indexes for memory entries during memory writing, along with recording\\nauxiliary information to assist in retrieval. During memory reading, matching scores are calculated\\nfor each memory entry, and the top-K entries will be used for the decision-making process of agents.\\n15'), Document(metadata={'source': 'survey.pdf', 'page': 15, 'page_label': '16'}, page_content='In existing studies, most agents utilize retrieval methods to process the memory information. For\\nexample, Park et al. [83] first calculate the relevance between the current context and memory entries\\nby cosine similarity, and obtain the importance and recency according to auxiliary information.\\nMemoryBank [6] employs a dual-tower dense retrieval model to find related information from past\\nconversations. Each memory entry is encoded into an embedding and subsequently indexed by\\nFAISS [124] to improve the efficiency of retrieval. When reading memories, the current context will\\nbe encoded as representations to obtain the most relevant memory. Moreover, RET-LLM [7] intends\\nto design a write-read memory module for general usage. It utilizes Locality-Sensitive Hashing\\n(LSH) to retrieve tuples with relative entries in the database to provide more information. In addition,\\nChatDB [96] designs to utilize symbolic memory, and proposes to generate SQL statements to retrieve\\nfrom database to obtain stored information.\\nThe retrieval methods considerably depend on the accuracy and efficiency of obtaining expected\\ninformation. An inaccurate retrieval strategy can potentially acquire unrelated information that is\\nunhelpful for agent inference. And a heavy retrieval system can lead to large computational costs\\nand long time latency, especially when handling massive information. Moreover, retrieval methods\\ntypically store homogeneous information inside the environment, where all the information is in a\\nconsistent form. For heterogeneous information outside the environment, it’s difficult to directly\\napply the same method for memory storage.\\nExternal Knowledge. To obtain more information, some agents acquire external knowledge by\\ninvoking tools, with the aim of transforming additional relevant knowledge into their own memories\\nfor decision-making. For instance, accessing external knowledge through Application Programming\\nInterface (API) is a common practice [ 104, 5]. Nowadays, abundant public information, such as\\nWikipedia and OpenWeatherMap4, are available online (either free of charge or on a paying basis),\\nand can be conveniently accessed through API calls. For instance, in [Step 2] of task (A) of the\\nexample in Section 3.1, external knowledge from the digital magazine is obtained with tool methods.\\nIn existing models, Toolformer [125] proposes to teach LLM to use tools, which can acquire external\\nknowledge for better solving tasks. Furthermore, ToolLLM [126] empowers Llama [127] with the\\nability to utilize more APIs in RapidAPI5 and to enable multi-tool usage, which provides a general\\ninterface to extend agents’ ability. In TPTU [110], the agents are incorporated in both task planning\\nand tool usage, in order to tackle intricate problems. The follow-up work [ 111] further improves\\nits ability extensively like retrieval. In ToRA [128], the agents are required to solve mathematical\\nproblems. They utilize imitation learning to improve their ability to use program-based tools.\\nThe above methods significantly advance the capabilities of agents by allowing them to access\\nexternal up-to-date and real-world information from diverse sources. However, the reliability of\\nthis information can be questionable due to potential inaccuracies and biases [18]. Furthermore, the\\nintegration of tools into agents demands a comprehensive understanding to interpret the retrieved\\ninformation across various contexts, which can incur higher computational costs and complications\\nin aligning external data with internal decision-making processes. Additionally, utilizing external\\nAPIs brings forth concerns regarding privacy, data security, and compliance with usage policies,\\nnecessitating rigorous management and oversight [18].\\n5.2.2 Memory in Parametric Form\\nAn alternative type of approaches is to represent memory in parametric form. They do not take up\\nthe extra length of context in prompts, so they are not constrained by the length limitations of LLM\\ncontext. However, the parametric memory form is still under-researched, and we categorize previous\\nworks into two types: fine-tuning methods and memory editing methods.\\nFine-tuning Methods. Integrating external knowledge into the memory of agents is beneficial\\nfor enriching domain-specific knowledge on top of its general knowledge. To infuse the domain\\nknowledge into LLMs, supervised fine-tuning is a common approach, which empowers agents with\\nthe memory of domain experts. It significantly improves the agent’s ability to accomplish domain-\\nspecific tasks. In task (A) of the example in Section 3.1, the external knowledge of attractions from\\nmagazines can be fine-tuned into the parameters of LLMs prior to this task.\\n4https://openweathermap.org\\n5https://rapidapi.com/hub\\n16'), Document(metadata={'source': 'survey.pdf', 'page': 16, 'page_label': '17'}, page_content='In previous works, Character-LLM [105] focuses on the role-play circumstance. It utilizes supervised\\nfine-tuning strategies with role-related data ( e.g., experiences), to endow agents with the specific\\ntraits and characteristics of the role. Huatuo [ 107] intends to empower agents with professional\\nability in the biomedical domain. It tries to fine-tune Llama [127] on Chinese medical knowledge\\nbases. Besides, in order to create artificial doctors, DoctorGLM [ 129] fine-tunes ChatGLM [130]\\nwith LoRA [131], and Radiology-GPT [ 132] improves domain knowledge on radiology analysis\\nby supervised fine-tuning on an annotated radiology dataset. Moreover, InvestLM [ 113] collects\\ninvestment data and fine-tunes it to improve domain-specific abilities on financial investment.\\nThe fine-tuning methods can effectively bridge the gap between general agents and specialized\\nagents. It improves the capability of agents on the tasks that require high accuracy and reliability on\\ndomain-specific information. Nevertheless, fine-tuning LLMs for specific domains could potentially\\nlead to overfitting, and it also raises concerns about catastrophic forgetting, where LLMs may forget\\nthe original knowledge because of updating their parameters. Another limitation of fine-tuning lies\\nin the computational cost and time consumption, as well as the requirement of a large amount of\\ndata. Therefore, most fine-tuning approaches are applied to offline scenarios, and can seldom deal\\nwith online scenarios, such as fine-tuning with agent observations and trial experiences. Due to\\nthe frequent agent-environment interactions, it is unaffordable for the cost of backpropagation to\\nfine-tune every step of the online and dynamic interactions.\\nMemory Editing Methods. Apart from the fine-tuning approaches, another type of methods for\\ninfusing memory into model parameters is knowledge editing [133, 134]. Unlike fine-tuning methods\\nthat extract patterns from certain datasets, knowledge editing methods specifically target and adjust\\nonly the facts that need to be changed. It ensures that unrelated knowledge remains unaffected.\\nKnowledge editing methods are more suitable for small-scale memory adjustments. Generally, they\\nhave lower computational costs, making them more suitable for online scenarios. In our example of\\ntask (B), Alice always watches movies at 9:00 PM from the agent’s memory, but she may recently\\nchange her work and would not be empty at 9:00 PM. If so, the related memory (such as routines at\\n9:00 PM) should be edited, which can be implemented by knowledge editing methods.\\nIn previous studies, MAC [ 106] intends to design an effective and efficient memory adaptation\\nframework for online scenarios. It utilizes meta-learning to substitute the optimization step. Per-\\nsonalityEdit [135] focuses on editing the personality of LLMs and agents, where it changes their\\ntraits based on theories such as the big-five factor. MEND [134] utilizes the idea of meta-learning\\nto train a lightweight model, which is capable of generating modifications for model parameters of\\na pre-trained language model. APP [ 136] studies whether adding a new fact leads to catastrophic\\nforgetting of existing facts. It focuses on the impact of neighbor perturbation on memory addition.\\nMoreover, KnowledgeEditor [133] trains a hyper-network to predict the modification of model pa-\\nrameters when injecting memory based on a learning-to-update problem formulation. Wang et al.\\n[137] propose a new optimization target to change the poisoning knowledge of LLM, and maintain\\nthe general performance at the same time. For LLM-based agents, the agents can change bad memory\\nby knowledge editing, which can be considered as a type of forgetting mechanism.\\nKnowledge editing methods provide an innovative way to update the information stored within the\\nparameters of LLMs. By specifically targeting and adjusting the facts, these methods can ensure\\nthe non-targeted knowledge unaffected during updates, thus mitigating the issue of catastrophic\\nforgetting. Moreover, the targeted adjustment mechanism allows for more efficient and less resource-\\nintensive updates, making knowledge editing an appealing choice for high-precision and real-time\\nmodifications. However, despite these promising developments, computational costs of meta-training\\nand the preservation of unrelated memories remain significant challenges.\\n5.2.3 Advantages and Disadvantages of Textual and Parametric Memory\\nTextual memory and parametric memory have their strengths and weaknesses respectively, making\\nthem suitable for different memory contents and application scenarios. In this section, we discuss the\\nadvantages and disadvantages of these two forms of memory from various aspects.\\nEffectiveness. The textual memory stores raw information about the agent-environment interactions,\\nwhich is more comprehensive and detailed. However, it is constrained by the token limitation\\nof LLM prompts, which makes the agent hard to store extensive information. In contrast, the\\nparametric memory is not limited by the prompt length, but it may suffer from information loss when\\ntransforming texts into parameters, and the complex memory training can bring additional challenges.\\n17'), Document(metadata={'source': 'survey.pdf', 'page': 17, 'page_label': '18'}, page_content='Efficiency. For textual memory, each LLM inference requires to integrate memory into the context\\nprompt, which leads to higher costs and longer processing times. In contrast, for parametric memory,\\nthe information can be integrated into the parameters of the LLM, eliminating the extra costs of these\\ncontexts. However, parametric memory takes additional costs in the writing process, but textual\\nmemory is easier to write, especially for small amounts of data. In a nutshell, textual memory is more\\nefficient in writing, while parametric memory is more efficient in reading.\\nInterpretability. Textual memory is usually more explainable than the parametric one, since natural\\nlanguages are the most natural and straightforward strategies for humans to understand, while\\nparametric memory is commonly represented in latent space. Nevertheless, such explainability is\\nobtained at the cost of information density. This is because the sequences of words in textual memory\\nare represented in a discrete space, which is not as dense as continuous space in parametric memory.\\nIn conclusion, the trade-offs between these two types of memories make them suitable for different\\napplications. For example, for the tasks that require recalling recent interactions, like conversational\\nand context-specific tasks, textual memory seems more effective. For the tasks that require a large\\namount of memory, or well-established knowledge, parametric memory can be a better choice.\\n5.3 Memory Operations\\nWe separate the entire procedure of memory into three operations: memory writing, memory manage-\\nment, and memory reading. These three typically collaborate to achieve memory function, providing\\ninformation for LLM inference. We summarize previous works on memory operations in Table 3.\\n5.3.1 Memory Writing\\nAfter the information is perceived by the agent, a part of it will be stored by the agent for further usage\\nthrough the memory writing operation, and it is crucial to recognize which information is essential to\\nstore. Many studies choose to store the raw information, while others also put the summary of the\\nraw information into the memory module.\\nRepresentative Studies. In TiM [97], the raw information will be extracted as the relation between\\ntwo entities, and stored in a structured database. When writing into the database, similar contents will\\nbe stored in the same group. In SCM [98], it designs a memory controller to decide when to execute\\nthe operations. The controller serves as a guide for the whole memory module. In MemGPT [100],\\nthe memory writing is entirely self-directed. The agents can autonomously update the memory based\\non the contexts. In MemoChat [94], the agents summarize each conversation segment by abstracting\\nthe mainly discussed topics and storing them as keys for indexing memory pieces.\\nDiscussion. Previous research indicates that designing the strategy of information extraction during\\nthe memory writing operation is vital [94]. This is because the original information is commonly\\nlengthy and noisy. Besides, different environments may provide various forms of feedback, and how\\nto extract and represent the information as memory is also significant for memory writing.\\n5.3.2 Memory Management\\nFor human beings, memory information is constantly processed and abstracted in the brains. The\\nmemory in the agent can also be managed by reflecting to generate higher-level memories, merging\\nredundant memory entries, and forgetting unimportant, early memories.\\nRepresentative Studies. In MemoryBank [6], the agents process and distill the conversations into a\\nhigh-level summary of daily events, similar to how humans recall key aspects of their experiences.\\nThrough long-term interactions, they continually evaluate and refine their knowledge, generating daily\\ninsights into personality traits. In V oyager [99], the agents are able to refine their memory based on\\nthe feedback of the environment. In Generative Agents [83], the agents can reflect to get higher-level\\ninformation, where the abstract thoughts are generated from agents. The reflection process will be\\nactivated when there are accumulated events that are enough to address. For GITM [93], in order to\\nestablish common reference plans for various situations, key actions from multiple plans are further\\nsummarized in the memory module.\\nDiscussion. Most of the memory management operations are inspired by the working mechanism of\\nhuman brains. With the strong capability of LLMs to simulate human minds, these operations can\\nhelp the agents to better generate high-level information and interact with environments.\\n18'), Document(metadata={'source': 'survey.pdf', 'page': 18, 'page_label': '19'}, page_content='Table 3: Summarization of the memory operations. If a model does not have special designs on the\\nmemory operations, we use ◦ to label it, otherwise, it is denoted by ✓. × means that the memory\\noperations are not discussed in the paper.\\nModels Writing\\nManagement\\nReadingMerging Reflection Forgetting\\nMemoryBank [6] ✓ ✓ ✓ ✓ ✓\\nRET-LLM [7] ✓ × × × ✓\\nChatDB [96] ✓ × ✓ × ✓\\nTiM [97] ✓ ✓ × ✓ ✓\\nSCM [98] ✓ ✓ × × ✓\\nV oyager [99] ✓ × ✓ × ✓\\nMemGPT [100] ✓ × ✓ × ✓\\nMemoChat [94] ✓ × × × ✓\\nMPC [101] ✓ × × × ✓\\nGenerative Agents [83] ✓ × ✓ ✓ ✓\\nRecMind [102] ◦ × × × ✓\\nRetroformer [103] ✓ ✓ ✓ × ◦\\nExpeL [82] ✓ ✓ ✓ × ◦\\nSynapse [91] ✓ × × × ✓\\nGITM [93] ◦ ✓ ✓ × ✓\\nReAct [104] ◦ × × × ◦\\nReflexion [5] ✓ ✓ ✓ × ◦\\nRecAgent [95] ✓ ✓ ✓ ✓ ✓\\nCharacter-LLM [105] ✓ × × × ◦\\nMAC [106] ✓ ✓ ✓ × ✓\\nHuatuo [107] ✓ × × × ◦\\nChatDev [1] ✓ × ✓ × ✓\\nInteRecAgent [108] ✓ ✓ ✓ × ✓\\nMetaAgents [109] ✓ × ✓ × ✓\\nTPTU [110, 111] ◦ × ✓ × ✓\\nMetaGPT [112] ✓ × ✓ × ✓\\nS3 [2] ✓ × ✓ ✓ ✓\\nInvestLM [113] ✓ × × × ◦\\n5.3.3 Memory Reading\\nWhen the agents require information for reasoning and decision-making, the memory reading\\noperation will extract related information from memory for usage. Therefore, how to access the\\nrelated information for the current state is important. Due to the massive quantity of memory entities,\\nand the fact that not all of them are pertinent to the current state, careful design is required to extract\\nuseful information based on relevance and other task-orientated factors.\\nRepresentative Studies. In ChatDB [96], the memory reading operation is executed by the SQL\\nstatements. These statements will be generated by agents as a series of Chain-of-Memory in advance.\\nIn MPC [101], the agents can retrieve relevant memory from the memory pool. This method also\\nproposes to provide Chain-of-Thought examples for ignoring certain memory. ExpeL [82] utilizes\\nthe Faiss [124] vector store as the pool of memory, and obtains the top-K successful trajectories that\\nshare the highest similarity scores with the current task.\\nDiscussion. To some extent, the memory reading and writing operations are collaborative, and the\\nforms of memory writing greatly influence the methods of memory reading. For the forms of textual\\n19'), Document(metadata={'source': 'survey.pdf', 'page': 19, 'page_label': '20'}, page_content='Figure 5: An overview of the evaluation methods of the memory module.\\nmemory, most previous works use the text similarity and other auxiliary information for reading.\\nFor the forms of parametric memory, existing models may just utilize the updated parameters for\\ninference, which can be seen as an implicit reading process.\\n6 How to Evaluate the Memory in LLM-based Agent\\nHow to effectively evaluate the memory module remains an open problem, where diverse evaluation\\nstrategies have been proposed in previous works according to different applications. To clearly\\nshow the common ideas of different evaluation methods, in this section, we summarize a general\\nframework, which includes two broad evaluation strategies (see Figure 5 for an overview), that\\nis, (1) direct evaluation, which independently measures the capability of the memory module. (2)\\nindirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\\nbe effectively accomplished, the memory module is demonstrated to be useful.\\n6.1 Direct Evaluation\\nThis type of approaches regards the memory of the agents as a stand-alone component and evaluates\\nits effectiveness independently. Previous studies can be categorized into two classes: subjective\\nevaluation and objective evaluation. The subjective evaluation aims to measure memory effectiveness\\nbased on human judgments, which can be widely used in the scenarios that lack objective ground\\ntruths. Objective evaluation assesses memory effectiveness based on numerical metrics, which makes\\nit easy to compare different memory modules.\\n6.1.1 Subjective Evaluation\\nIn subjective evaluation, there are two key problems, that is, (1) what aspects should be evaluated\\nand (2) how to conduct the evaluation process. To begin with, the following two aspects are the most\\ncommon perspectives leveraged to evaluate the memory module.\\nCoherence. This aspect refers to whether the recalled memory is natural and suitable for the current\\ncontext. For example, if the agent is making a plan for Alice’s travel, the memory should be related\\nto her preference for traveling rather than working. In previous works, Modarressi et al. [7] study\\nwhether the memory module could provide proper references among the ever-changing knowledge.\\nLiang et al. [98] present some examples to demonstrate the relation between the current query and\\nhistorical memory. Zhong et al. [6] and Liu et al. [97] assess the coherence of responses that integrate\\ncontext and retrieved memory by scoring labels. Lee et al. [101] focus on the contradiction between\\nthe recalled memory and contexts.\\nRationality. This aspect aims to evaluate whether the recalled memory is reasonable. For example, if\\nthe agent is asked to answer \"Where is the Summer Palace\", the recalled memory should be \"The\\nSummer Palace is in Beijing\" rather than \"The Summer Palace is on the Moon\". In previous works,\\nLee et al. [101] ask crowd workers to directly score the rationality of the retrieved memory. Zhong\\net al. [6] and Liu et al. [97] recruit human evaluators to check if the memory contains reasonable\\nanswers for the current question.\\nAs for how to conduct the evaluation process, there are two important problems. The first one is how\\nto select the human evaluators. In general, the evaluators should be familiar with the evaluation task,\\nwhich ensures that the labeling results are convincing and reliable. In addition, the backgrounds of\\nthe evaluators should be diverse to remove subjective biases of specific human groups. The second\\nproblem is how to label the outputs of the memory module. Usually, one can either directly score the\\n20'), Document(metadata={'source': 'survey.pdf', 'page': 20, 'page_label': '21'}, page_content='results [6] or make comparisons between two candidates [95]. The former can obtain absolute and\\nquantitative evaluation results, while the latter can remove the labeling noises when independently\\nscoring each candidate. In addition, the granularity of the ratings should also be carefully designed.\\nToo coarse ratings may not effectively discriminate the capabilities of different memory modules,\\nwhile too fine-grained ones may bring more effort for the workers to make judgments.\\nIn general, subjective evaluation can be used in a wide range of scenarios, where one just needs\\nto define the evaluation aspects and let recruited workers make judgments. This method is usually\\nmore explainable since the workers can provide the reasons for their judgments. However, subjective\\nevaluation is costly due to the need to employ human evaluators. Additionally, different groups of\\nevaluators may have various biases, making the results difficult to reproduce and compare.\\n6.1.2 Objective Evaluation\\nIn objective evaluation, previous work usually defines numeric metrics to evaluate the effectiveness\\nand efficiency of the memory module.\\nResult Correctness. This metric measures whether the agent can successfully answer pre-defined\\nquestions directly based on the memory module. For example, the question could be \"Where did\\nAlice go today?\" with two choices \"A: the Summer Palace\" and \"B: the Great Wall\". Then, the agent\\nshould choose the correct answer based on the problem and its memory. The agent-generated answer\\nwill be compared with the ground truth. Formally, the accuracy can be calculated as\\nCorrectness = 1\\nN\\nNX\\ni=1\\nI [ai = ˆai] ,\\nwhere N is the number of problems, ai represents the ground truth for the i-th problem, ˆai means the\\nanswer given by the agent, and I [ai = ˆai] is the matching function commonly represented as\\nI [ai = ˆai] =\\n\\x1a1 if ai = ˆai,\\n0 if ai ̸= ˆai.\\nIn previous works, Hu et al. [96] construct questions from past histories with annotated ground\\ntruths and calculate the accuracy of whether the recalled memory could match the correct answers.\\nSimilarly, Packer et al. [100] generate questions and answers that can only be derived from past\\nsessions, and compare the responses from the agents with the ground truths to calculate the accuracy.\\nReference Accuracy. This metric evaluates whether the agent can discover relevant memory contents\\nto answer the questions. Different from the above metric, which focuses on the final results, reference\\naccuracy cares more about the intermediate information to support the agent’s final decisions. In\\nspecific, it compares the retrieved memory with the pre-prepared ground truth. For the above problem\\nof \"Where did Alice go today?\", if the memory contents include (A) \"Alice had lunch with friends at\\nWangfujing today.\" and (B) \"Alice had roast duck for lunch\", then a better memory module should\\nselect (A) as a reference to answer the question. Usually, researchers leverage F1-score to evaluate\\nthe reference accuracy, which is calculated as\\nF1 = 2 · Precision · Recall\\nPrecision + Recall,\\nwhere the precision and recall scores are calculated as Precision = TP\\nTP+FP and Recall = TP\\nTP+FN . The\\nTP represents the number of true positive memory contents, FP means the number of false positive\\nmemory contents, and FN indicates the number of false negative memory contents. In previous works,\\nLu et al. [94] utilize F1-score to evaluate the retrieval process of the memory, and Zhong et al.[6]\\nfocus on assessing whether related memory can be successfully retrieved.\\nResult Correctness and Reference Accuracy are both utilized to evaluate the effectiveness of the\\nmemory module. Beyond effectiveness, efficiency is also an important aspect, especially for real-\\nworld applications. Therefore, we describe the evaluation of efficiency as follows.\\nTime & Hardware Cost. The total time cost includes the time leveraged for memory adaption and\\ninference. The adaptation time refers to the time of memory writing and memory management, while\\nthe inference time indicates the time latency of memory reading. In specific, the difference from the\\n21'), Document(metadata={'source': 'survey.pdf', 'page': 21, 'page_label': '22'}, page_content='end time to the start time of memory operations can be considered as the time consumption. Formally,\\nthe average time consumption of each type of operation can be represented as\\n∆time = 1\\nM\\nMX\\ni=1\\ntend\\ni − tstart\\ni ,\\nwhere M represents the number of these operations, tend\\ni means the end time of the i-th operation, and\\ntstart\\ni indicates the start time of that operation. As for the computation overhead, it can be evaluated\\nby the peak GPU memory allocation. In previous works, Tack et al. [106] utilize the peak memory\\nallocation and adaptation time to assess the efficiency of memory operations.\\nObjective evaluation offers numeric strategies to compare different methods of memory, which is\\nimportant to benchmark this field and promote future developments.\\n6.2 Indirect Evaluation\\nBesides the above method that directly evaluates the memory module, evaluating via task completion\\nis also a popular evaluation strategy. The intuition behind this type of approaches is that if the\\nagent can successfully complete a task that highly depends on memory, it suggests that the designed\\nmemory module is effective. In the following parts, we present several representative tasks that are\\nleveraged to evaluate the memory module in indirect ways.\\n6.2.1 Conversation\\nEngaging in conversations with humans is one of the most important applications of agents, where\\nmemory plays a crucial role in this process. By storing context information in memory, the agents\\nallow users to experience personalized conversations, thus improving users’ satisfaction. Therefore,\\nwhen other parts of the agents are determined, the performance of the conversation tasks can reflect\\nthe effectiveness of different memory modules.\\nIn the context of conversation, consistency and engagement are two commonly used methods to\\nevaluate the effectiveness of the agents’ memory. Consistency refers to how the response from agents\\nis consistent with the context because dramatic changes should be avoided during the conversation.\\nFor example, Lu et al. [94] evaluate the consistency of agents on interactive dialogues, using GPT-4\\nto score on the responses from agents. Engagement refers to how the user is engaged to continue the\\nconversation. It reflects the quality and attraction of agents’ responses, as well as the ability of agents\\nto craft the personas for current conversations. For example, Lee et al. [101] assess the engagingness\\nof responses by SCE-p score, and Packer et al. [100] utilize CSIM score to evaluate the memory\\neffect on increasing engagement of users.\\n6.2.2 Multi-source Question-answering\\nMulti-source questing-answering can comprehensively evaluate the memorized information from\\nmultiple sources, including inside-trial information, cross-trial information, and external knowledge.\\nIt focuses on the integration of memory utilization from various contents and sources.\\nIn previous works, Yao et al. [104] evaluate the memory that integrates information from the task\\ntrial and the external knowledge from Wikipedia. Then, Shinn et al. [5] and Yao et al. [103] further\\ninclude the cross-trial information of the same task, where the memory is permitted to obtain more\\nexperiences from previous failed trials. Moreover, Packer et al. [100] allow agents to utilize the\\nmemory from multi-document information for question-answering.\\nBy evaluating multi-source question-answering tasks, the memory of agents can be examined on\\nthe capability of content integration from various sources. It also reveals the issue of the memory\\ncontradiction due to multiple information sources, and the problem of updated knowledge, which can\\npotentially affect the performance of the memory module.\\n6.2.3 Long-context Applications\\nBeyond the above general applications, in many scenarios, LLM-based agents have to make decisions\\nbased on extremely long prompts. In these scenarios, the long prompts are usually regarded as the\\nmemory contents, which play an important role in driving agent behaviors.\\n22'), Document(metadata={'source': 'survey.pdf', 'page': 22, 'page_label': '23'}, page_content='In previous works, Huang et al. [19] organize a comprehensive survey for long-context LLMs, which\\nprovides a summary of evaluation metrics on long-context scenarios. Moreover, Shaham et al. [138]\\npropose a zero-shot benchmark for evaluating agents’ understanding of long-context natural languages.\\nAs for specific long-context tasks, long-context passage retrieval is one of the important tasks for\\nevaluating the long-context ability of agents. It requires agents to find the correct paragraph in a long\\ncontext that corresponds to the given questions or descriptions [139]. Long-context summarization\\nis another representative task. It requests agents to formulate a global understanding of the whole\\ncontext, and summarizes it according to the descriptions, where some metrics on matching scores\\nlike ROUGE can be utilized to compare the results with ground truths.\\nThe evaluation of long-context applications provides broader approaches to assess the function of\\nmemory in agents, focusing on practical downstream scenarios. The comprehensive benchmarks [138,\\n140] also provide an objective assessment for the ability of long-context understanding.\\n6.2.4 Other Tasks\\nIn addition to the above three types of major tasks for indirect evaluation, there are also some other\\nmetrics in general tasks that can reveal the effectiveness of the memory module.\\nSuccess rate refers to the proportion of tasks that agents can successfully solve. For Yao et al.\\n[104], Shinn et al. [5] and Zhao et al. [82], they assess how many spacial tasks can be correctly\\ncompleted through reasoning and memory in AlfWorld [141]. In Zhu et al. [93], they evaluate the\\nsuccess rate of producing different items in Minecraft to show the effect of memory. Moreover, Shinn\\net al. [5] measure the success rate of passed problems by generated codes, and Zheng et al. [91]\\ncalculate the success rate of computer control and accuracy of element selection to show the function\\nof trajectory-as-exemplar memory. Exploration degree typically appears in exploratory games, which\\nreflects the extent that agents can explore the environment. For example, Wang et al.[99] compare\\nthe numbers of distinct items explored in Minecraft to reflect the skill learning in memory.\\nIn fact, nearly all the memory-equipped agents can evaluate the effect of memory by ablation studies,\\ncomparing the performance between with/without memory modules. The evaluation on specific\\nscenarios can better reflect the significance of memory for the downstream applications practically.\\n6.3 Discussions\\nCompared with direct evaluation, indirect evaluation via specific tasks can be easier to conduct, since\\nthere are already many public benchmarks. However, the performance on tasks can be attributed\\nto various factors, and memory is only one of them, which may make the evaluation results biased.\\nBy direct evaluation, the effectiveness of the memory module can be independently evaluated,\\nwhich improves the reliability of the evaluation results. However, to our knowledge, there are no\\nopen-sourced benchmarks tailored for the memory modules in LLM-based agents.\\n7 Memory-enhanced Agent Applications\\nRecently, LLM-based agents have been investigated across a wide variety of scenarios, facilitating\\nsocietal advancement. In general, most LLM-based agents are equipped with memory modules.\\nHowever, the specific effects undertaken by these memory components, the particular information\\nthey store, and the implementation methods they use, vary across different applications. In order to\\nprovide insights for the design of memory functionalities in LLM-based agents, in this section, we\\nreview and summarize how memory mechanisms are manifested in LLM-based agents across various\\napplication scenarios. In specific, we categorize them into several classes: role-playing and social\\nsimulation, personal assistant, open-world games, code generation, recommendation, expert systems\\nin specific domains, and other applications. The summarization is shown in Table 4.\\n7.1 Role-playing and Social Simulation\\nRole-playing represents a classic application of LLM-based agents, where memory plays a crucial role\\ninside the agents. It endows roles with distinct characteristics, differentiating them from one another.\\nMany previous studies have explored methods for constructing role memories [105, 143, 145–147].\\nShao et al. [105] construct the memory of roles by experience uploading, which utilizes SFT to\\ninject memory into model parameters. Li et al. [143] enhance large language models for role-playing\\nvia an improved prompt and the character memory extracted from scripts, where user queries and\\n23'), Document(metadata={'source': 'survey.pdf', 'page': 23, 'page_label': '24'}, page_content='Table 4: Summarization of memory-enhanced agents applications.\\nApplications Models Applications Models\\nRole-playing\\nCharacter-LLM [105]\\nCode Generation\\nRTLFixer [142]\\nChatHaruhi [143] GameGPT [144]\\nRoleLLM [145] ChatDev [1]\\nNarrativePlay [146] MetaGPT [109]\\nCharacterGLM [147] CodeAgent [114]\\nSocial Simulation\\nGenerative Agents [83]\\nRecommendation\\nRecAgent [95]\\nLyfe Agents [148] InteRecAgent [108]\\nS3 [2] RecMind [102]\\nMetaAgents [109] AgentCF [149]\\nWarAgent [150]\\nMedicine\\nHuatuo [107]\\nPersonal Assistant\\nMemoryBank [6] DoctorGLM [129]\\nRET-LLM [7] Radiology-GPT [132]\\nMemoChat [94] Wang et al. [151]\\nMemGPT [100] EHRAgent [152]\\nMPC [101] ChatDoctor [115]\\nAutoGen [153]\\nFinance\\nInvestLM [113]\\nChatDB [96] TradingGPT [154]\\nTiM [97] QuantAgent [155]\\nSCM [98] FinMem [156]\\nGame\\nV oyager [99] Koa et al. [157]\\nGITM [93]\\nScience\\nChemist-X [158]\\nJARVIS [159] ChemDFM [160]\\nLARP [161] MatChat [162]\\nchatbot’s responses are concatenated to form a sequence as memory. Wang et al. [145] infuse\\nrole-specific knowledge and episode memories into LLM-based agents, where context QA pairs\\nare concatenated to form episode memory. Zhao et al. [146] aim to generate human-like responses,\\nguided by personality traits extracted from narratives, which can be stored and retrieved by relevance\\nand importance. Zhou et al. [147] generate character-based dialogues for different roles and empower\\nLLM-based agents with corresponding styles by SFT.\\nSocial simulation is basically an extension of role-playing, which focuses more on multi-agent\\nmodeling. The memory module is an important component for such applications, which helps to\\naccurately simulate human dynamic behaviors. In previous studies, Kaiya et al. [148] propose a\\nSummarize-and-Forget memory mechanism for better self-monitoring in social scenarios. Gao et al.\\n[2] focus on social network simulation systems. Each agent in the system has a memory pool, which\\nconsists of diverse user messages from online platforms to identify the user. Li et al. [163] maintain\\nconversation contexts, encompassing the economic environment and agent decisions from previous\\nmonths, in order to simulate the impact of broad macroeconomic trends on agents’ decision-making\\nand to make the agents grasp market dynamics. Li et al. [109] simulate the job-seeking scenario\\nin human society, where the memory of agents includes profiles and goals initially and is further\\nenriched with other information, like dialogues and personal reflections. Hua et al. [150] simulate the\\ndecisions and consequences of the participating countries in the wars, where the conversations of\\nagents are continuously maintained into memory.\\nThere are several insights in designing an agent’s memory for role-play and social simulation. First,\\nthe memory should be consistent with the roles’ characteristics, which can be used to identify each\\nrole and distinguish it from the others. This is crucial for improving the realism of role-play and the\\ndiversity of social simulation. Second, the memory should appropriately influence the subsequent\\n24'), Document(metadata={'source': 'survey.pdf', 'page': 24, 'page_label': '25'}, page_content='actions of the agent to ensure the consistency and rationality of its behaviors. Additionally, for\\nhumanoid agents, their memory mechanisms should align with the features of human memory, such\\nas forgetting and long/short-term memory, which should refer to the theories of cognitive psychology.\\n7.2 Personal Assistant\\nLLM-based agents are well-suited for creating personal assistants, such as agents capable of engaging\\nin long-term conversations with users [ 94, 101, 153], as well as those tasked with automatically\\nseeking information [164]. These agents often need to memorize previous dialogues to maintain\\nthe consistency, and remember critical styles and events to generate more personalized and relevant\\nresponses. Lu et al. [94] maintain the context consistency for dialogues by saving contents and\\ninformation of conversations, which helps to find proper relevant information by retrieval. Lee et al.\\n[101] summarize conversations to extract important information, store it, and retrieve it for future\\ninference. Pan et al. [164] focus on information-seeking tasks, which design memory modules to\\nstore user’s context information, and empower external knowledge with tool usage. Wu et al.[153]\\nretain important context as memory to maintain conversation consistency.\\nIn summary, most memory implementations for personal assistants adopt retrieval methods in textual\\nform, because they are better at finding relevant information from pieces of conversations. For the\\nmemory storage, the agent should remember the factual information during user-agent interactions,\\nas well as the personal style of users, in order to generate responses that are tailored to the user’s\\nsituation. Additionally, when recalling memories, the agent should identify and retrieve the memory\\nthat is relevant to the current query and context. This principle can enable the agent to correctly\\nunderstand the user’s requirement, and maintain the consistency in conversations.\\n7.3 Open-world Game\\nFor games and open-world exploration, LLM-based agents always maintain post observations as\\ntask contexts, and store experiences in previous successful trials. By leveraging past experiences,\\nagents can avoid making the same mistakes repeatedly and achieve a high-level understanding of\\nenvironments, thus exploring more effectively. Some of them can acquire external databases or\\nAPIs to obtain general knowledge [ 99, 93, 159, 161]. Wang et al. [99] save obtained skills into\\nmemory for further usage in Minecraft. Zhu et al. [93] store and retrieve successful trajectories\\nas examples for similar tasks, and utilize external Minecraft Wiki by API calls. Wang et al. [159]\\nconstruct multimodal memory as a knowledge library and provide examples for prompt by retrieving\\ninteractive experiences. Yan et al. [161] maintain working memory for decision-making, save and\\nretrieve relevant past experiences, and implement external datasets for general knowledge.\\nIn summary, no matter inside-trial or cross-trial information, the key aspect of memory is to reflect on\\npast interactions and draw experiences that can be applied to the subsequent exploration. In addition\\nto accumulating experience through self-involving trials, absorbing external knowledge as part of the\\nagent’s memory is also an important way to enhance the exploratory capabilities of the agent.\\n7.4 Code Generation\\nIn the scenario of code generation, LLM-based agents can search relevant information from the\\nmemory, thereby obtaining more knowledge for development. They can save previous experiences for\\nfuture problems, and also maintain context in conversational development interfaces [142, 144, 1, 109].\\nTsai et al. [142] construct an external non-parametric memory database, which stores the compiler\\nerrors and human expert instructions for automatic syntax error fixing. In [144], personal information\\nwill be stored in the memory, and helps in retaining context and knowledge for decision-making.\\nQian et al. [1] adopt multi-agents to develop software, where each role maintains a memory to store\\nthe past conversations with other roles. Li et al. [109] also focus on software development, and the\\nagent can retrieve its historical records preserved in memory when errors occur. Zhang et al. [114]\\ncan search relevant information when they face problems on code generation.\\nBy leveraging external resources, the agents can learn from code-related knowledge and store it\\ninto their memory, thereby enhancing the capabilities of code generation. In addition, the memory\\ncan improve the continuity and consistency in code generation. By integrating contextual memory,\\nthe agent can better understand the requirements for software development, thereby enhancing the\\ncoherence of the generated code. Furthermore, the memory is also crucial for the iterative optimization\\nof code, as it can identify the developer’s targets based on the histories.\\n25'), Document(metadata={'source': 'survey.pdf', 'page': 25, 'page_label': '26'}, page_content='7.5 Recommendation\\nIn the field of recommendation, some previous works focus on simulating users in recommender\\nsystems [ 95, 108], where the memory can represent the user profiles and histories in the real\\nworld. Others try to improve the performance of recommendation, or provide other formats of\\nrecommendation interfaces [149, 102]. Wang et al. [95] simulate user behaviors in recommendation\\nscenarios to generate data for recommender systems, and the agents store past observations and\\ninsights into a hierarchical memory. In Huang et al. [108], the memory in LLM-based agents can\\narchive the user’s conversational history over extended periods, as well as capture the most recent\\ndialogues pertinent to the current prompt, to simulate interactive recommender systems. It also\\nuses an actor-critic reflection to improve the robustness of agents. Item agents and user agents are\\nequipped with different memories in [149], where item agents are endowed with dynamic memory\\nmodules designed to capture and preserve information pertinent to their intrinsic attributes and the\\ninclinations of their adopters. For user agents, the adaptive memory updating mechanism plays a\\npivotal role in aligning the agents’ operations with user behaviors and preferences. Wang et al. [102]\\nmemorize individualized user information like reviews or ratings for items, and acquire domain-\\nspecific knowledge and real-time information by web searching tools.\\nFor both simulating users in recommender systems and capturing their preferences, retaining per-\\nsonalized information through memory is essential. A critical challenge lies in how to align the\\npersonalized information and feedback with LLMs, and store them into the memory of agents. It is\\nalso an important task for bridging the gap between conventional recommendation models and LLMs.\\n7.6 Expert System in Specific Domains\\nMedicine Domain. In the field of medicine, most of the previous works empower LLM-based agents\\nwith external knowledge in their memory [ 107, 129, 132, 151, 115]. Wang et al. [107] fine-tune\\nLLaMA [127] with medical knowledge graph CMeKG [ 165] in QA form, in order to enhance\\ntheir medical domain knowledge. Xiong et al. [129] adopt LoRA [ 131] to efficiently fine-tune\\non foundation models for healthcare. Wang et al. [151] empower LLM-based agents to acquire\\ntext-based external knowledge as reasoning reference. Besides, Shi et al. [152] build memory upon\\nthe most relevant successful cases from past experiences, and use similarity metric for the retrieval of\\nrelevant questions in the medicine domain.\\nFinance Domain. Some previous works also apply LLM-based agents in the finance domain, whose\\nmemory can store financial knowledge [113], market information [154, 156], and successful experi-\\nences [157, 155]. Yang et al. [113] construct financial investment dataset to fine-tune LLaMA [127]\\nto empower knowledge on investment. Li et al. [154] design a layered-memory structure to store\\ndifferent types of marketing information. Wang et al. [155] record the ongoing interaction like\\nexchanges and information to ensure consistent response, and record prior outputs as experiences for\\nretrieving relevant examples to provide a diverse learning context for agents. Koa et al.[157] store\\npast price movement and explanations, and generate reflections on previous trials. Yu et al. [156]\\nadopt a layered memory mechanism to provide abundant information for reasoning.\\nScience. In the domain of science, some existing works design LLM-based agents with a large\\namount of knowledge in memory to solve problems [ 158, 160, 162]. Chen et al. [158] include\\nmolecule database and online literature as external knowledge for memory in LLM-based agents, and\\nretrieve them when they need related information. Zhao et al. [160] and Chen et al. [162] empower\\ndomain knowledge by fine-tuning in Chemistry and structured materials respectively.\\nTo build an expert system based on agents in a specific vertical domain, it is necessary to retain the\\ndomain-specific knowledge in their memory. However, there are several challenges. First, domain\\nknowledge is specialized and requires higher accuracy, leading to difficulties in constructing memory\\nstorage. Second, domain knowledge is often time-sensitive, which can become outdated in the\\nfuture. Therefore, the memory needs to be partially updated when some of the knowledge has been\\nout-of-date. Furthermore, the substantial volume of domain knowledge makes it difficult to recall\\nfrom memory based on the current query.\\n7.7 Other Applications\\nThere are some other applications of memory in LLM-based agents. Wang et al. [166] focus on the\\ntask of cloud root cause analysis, using memory to store framework rules, task requirements, tools\\n26'), Document(metadata={'source': 'survey.pdf', 'page': 26, 'page_label': '27'}, page_content='documentation, few-shot examples, and agent observations. Qiang et al. [167] solve the problem\\nof ontology matching. The agents save conversational dialogues and construct a rational database\\nfor retrieving external knowledge. Wen et al. [168] investigate autonomous driving, whose memory\\nmodule is constructed by a vector database and contains the experiences from past driving scenarios.\\nWang et al. [169] propose to improve user acceptance testing, which employs a self-reflection\\nmechanism. After each trial, the operation agent summarizes the conversation and updates the\\nmemory pool, until the goal of the current step is accomplished.\\nFor different applications, the focus of memory varies, as it inherently serves the downstream tasks.\\nTherefore, the design should also consider the requirements of tasks.\\n8 Limitations & Future Directions\\n8.1 More Advances in Parametric Memory\\nAt present, the memory of LLM-based agents is predominantly in textual form, especially for\\ncontextual knowledge such as observation records, trial experiences, and textual knowledge databases.\\nAlthough textual memory possesses the advantages of being interpretable and easy to expand and\\nedit, it also implies a sacrifice in efficiency compared to parametric memory. Essentially, parametric\\nmemory boasts a higher information density, expressing semantics through continuous real-number\\nvectors in a latent space, whereas textual memory employs a combination of tokens in a discrete\\nspace for semantic expression. Thus, parametric memory offers a richer expressive space, and its\\nsoft encoding is more robust compared to the hard-coded form of token sequences. Additionally,\\nparametric memory is more storage-efficient, where it does not require the explicit storage of extensive\\ntexts, similar to a knowledge compression process. As for the memory management, such as merging\\nand reflection, parametric memory does not necessarily design manual rules like textual memory\\ndoes, but can employ optimization methods to learn these processes implicitly. Moreover, pluggable\\nparametric memory is similar to a digital life card, capable of endowing agents with the requisite\\ncharacteristics. For example, Huatuo [107] aims to enhance agents with expertise in the biomedical\\nfield by refining the Llama [ 127] model on Chinese medical knowledge bases. MAC [ 106] is\\ndesigned to create a parametric memory adaptation framework suitable for online settings, employing\\nmeta-learning techniques to replace the traditional optimization phase.\\nAlthough parametric memory holds great prospects, it currently faces numerous challenges. Foremost\\namong these is the issue of efficiency: how to effectively transform textual information into parameters\\nor modifications of parameters is a critical question. Presently, researchers can transfer vast amounts\\nof domain knowledge into the parameters of LLMs by SFT. However, it is time-consuming and\\nrequires extensive text corpus, making it unsuitable for situational knowledge. One viable approach\\nis to employ meta-learning to let models learn to memorize. For example, MEND [134] leverages the\\nmethod of meta-learning to train a compact model that has the ability to produce adjustments for the\\nparameters of a pre-trained language model. Moreover, the lack of interpretability associated with\\nparametric memory can be a hindrance, especially in domains requiring high levels of trust, such as\\nmedicine. Therefore, enhancing the credibility and interpretability of parametric memory is an urgent\\nissue that needs to be addressed.\\n8.2 Memory in LLM-based Multi-agent Applications\\nThe exploration of memory mechanisms within LLMs has burgeoned into the dynamic domain of\\nmulti-agent systems (MAS), marking significant advancements in the realms of synchronization,\\ncommunication, and the management of information asymmetry. One pivotal aspect that emerges in\\nthe cooperative scenarios is memory synchronization among agents. This process is fundamental\\nfor establishing a unified knowledge base, ensuring consistency in decision-making across different\\nagents. For example, Chen et al. [170] emphasize the significance of integrating synchronized\\nmemory modules for multi-robot collaboration. Another important aspect is the communication\\namong agents, which heavily relies on memory for maintaining context and interpreting messages.\\nFor example, Mandi et al. [171] illustrate memory-driven communication frameworks that foster a\\ncommon understanding among agents. In addition to cooperative scenarios, some studies also focus\\non competitive scenarios, and the information asymmetry becomes a crucial issue [172].\\nLooking ahead, the advancement of memory in LLM-based MAS is poised at the confluence of\\ntechnological innovation and strategic application. It beckons the exploration of novel memory\\n27'), Document(metadata={'source': 'survey.pdf', 'page': 27, 'page_label': '28'}, page_content='modules that can further enhance agent synchronization, enable more effective communication, and\\nprovide strategic advantages in information-rich environments. The development of such memory\\nmodels would not only necessitate addressing the current challenges of memory integration and man-\\nagement, but also explore the untapped potentials of memory in facilitating more robust, intelligent,\\nand adaptable MAS. As evidenced by pioneering research, the evolving landscape of LLM-based\\nMAS sets a promising stage for future innovations in memory utilization and management. This\\nexploration is expected to unravel new dimensions of memory integration, pushing the boundaries of\\nwhat is currently achievable and setting new benchmarks in the realm of MAS.\\n8.3 Memory-based Lifelong Learning\\nLifelong learning is an advanced topic in artificial intelligence, extending the learning capabilities of\\nagents across their life-long span [173]. Agents can continuously interact with their environment,\\npersistently observe environments, and acquire external knowledge, enabling a mode of enhancement\\nlike humans. The memory of an agent is key to achieving lifelong learning, as it needs to learn to store\\nand apply the past observations. Lifelong learning in LLM-based agents holds significant practical\\nvalue, such as in long-term social simulations and personal assistance. However, it also faces several\\nchallenges. Firstly, lifelong learning is temporal, necessitating that an agent’s memory captures\\ntemporality. This temporality could cause interactions between memories, such as memory overlap.\\nFurthermore, due to the extended period of lifelong learning, it needs to store a vast amount of\\nmemories and retrieve them when needed, possibly incorporating a certain mechanism for forgetting.\\n8.4 Memory in Humanoid Agent\\nA humanoid agent refers to an agent designed to exhibit behaviors consistent with humans, thereby\\nfacilitating applications in social simulation, studies of human behavior, and role-playing. Unlike\\ntask-oriented agents where greater capability is typically preferred, the proficiency of a humanoid\\nagent should closely mimic that of humans. Consequently, the memory of humanoid agents should\\nalign with human cognitive processes, adhering to psychological principles such as memory distortion\\nand forgetfulness. Additionally, humanoid agents should possess knowledge boundaries, meaning that\\ntheir knowledge should correspond to that of the entity they replicate. For instance, in role-playing\\nscenarios, an agent embodying a child should not possess an understanding of advanced mathematical\\nconcepts or other complex knowledge beyond what is typical for that age [174].\\n9 Conclusion\\nIn this survey, we provide a systematical review on the memory mechanism of LLM-based agents,\\nwhere we focus on three key problems including \"What is\", \"Why do we need\" and \"How to design\\nand evaluate\" the memory module in LLM-based agents. To show the importance of the agent’s\\nmemory, we also present many typical applications, where the memory module plays an important\\nrole. We believe this survey can offer valuable references for newcomers to this domain, and also\\nhope it can inspire more advanced memory mechanisms to enhance LLM-based agents.\\nAcknowledgement\\nWe thank Lei Wang for his proofreading and valuable suggestions to this survey.\\nReferences\\n[1] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\\nand Maosong Sun. Communicative agents for software development. arXiv preprint\\narXiv:2307.07924, 2023.\\n[2] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng\\nJin, and Yong Li. S3: Social-network simulation system with large language model-empowered\\nagents. arXiv preprint arXiv:2307.14984, 2023.\\n[3] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\\nJiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous\\nagents. arXiv preprint arXiv:2308.11432, 2023.\\n28'), Document(metadata={'source': 'survey.pdf', 'page': 28, 'page_label': '29'}, page_content='[4] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\\n[5] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference\\non Neural Information Processing Systems, 2023.\\n[6] Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. Memorybank: Enhancing large\\nlanguage models with long-term memory. arXiv preprint arXiv:2305.10250, 2023.\\n[7] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards\\na general read-write memory for large language models. arXiv preprint arXiv:2305.14322,\\n2023.\\n[8] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li,\\nRunyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A\\nsurvey. arXiv preprint arXiv:2308.10792, 2023.\\n[9] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei\\nWu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. arXiv preprint\\narXiv:2309.15025, 2023.\\n[10] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng\\nShang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. arXiv\\npreprint arXiv:2307.12966, 2023.\\n[11] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng,\\nYegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and\\nguideline for evaluating large language models’ alignment. arXiv preprint arXiv:2308.05374,\\n2023.\\n[12] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\\nand Haofen Wang. Retrieval-augmented generation for large language models: A survey.\\narXiv preprint arXiv:2312.10997, 2023.\\n[13] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing\\nfor large language models: A survey. arXiv preprint arXiv:2310.16218, 2023.\\n[14] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun\\nChen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportuni-\\nties. arXiv preprint arXiv:2305.13172, 2023.\\n[15] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\\nSiyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge\\nediting framework for large language models. arXiv preprint arXiv:2308.07269, 2023.\\n[16] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language\\nmodels: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint\\narXiv:2311.05876, 2023.\\n[17] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun\\nXi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge\\nediting for large language models. arXiv preprint arXiv:2401.01286, 2024.\\n[18] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\\nHuang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint\\narXiv:2304.08354, 2023.\\n[19] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan\\nYang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large\\nlanguage models: A comprehensive survey. arXiv preprint arXiv:2311.12351, 2023.\\n29'), Document(metadata={'source': 'survey.pdf', 'page': 29, 'page_label': '30'}, page_content='[20] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Ar-\\nmaghan Eshaghi. Beyond the limits: A survey of techniques to extend the context length in\\nlarge language models. arXiv preprint arXiv:2402.02244, 2024.\\n[21] Saurav Pawar, SM Tonmoy, SM Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The\\nwhat, why, and how of context length extension techniques in large language models–a detailed\\nsurvey. arXiv preprint arXiv:2401.07872, 2024.\\n[22] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip. Multimodal large\\nlanguage models: A survey. In 2023 IEEE International Conference on Big Data (BigData),\\npages 2247–2256. IEEE, 2023.\\n[23] Shezheng Song, Xiaopeng Li, and Shasha Li. How to bridge the gap between modalities: A\\ncomprehensive survey on multimodal large language model. arXiv preprint arXiv:2311.07594,\\n2023.\\n[24] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo\\nBaraldi, Marcella Cornia, and Rita Cucchiara. The (r) evolution of multimodal large language\\nmodels: A survey. arXiv preprint arXiv:2402.12451, 2024.\\n[25] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A\\nsurvey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\\n[26] Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi,\\nZiyang Yu, Mengdan Zhu, Yifei Zhang, et al. Beyond efficiency: A systematic survey of\\nresource-efficient large language models. arXiv preprint arXiv:2401.00625, 2024.\\n[27] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan,\\nYi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. Efficient large language models: A survey.\\narXiv preprint arXiv:2312.03863, 1, 2023.\\n[28] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen,\\nand Zhihao Jia. Towards efficient generative large language model serving: A survey from\\nalgorithms to systems. arXiv preprint arXiv:2312.15234, 2023.\\n[29] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient\\nfine-tuning methods for pretrained language models: A critical review and assessment. arXiv\\npreprint arXiv:2312.12148, 2023.\\n[30] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression\\nfor large language models. arXiv preprint arXiv:2308.07633, 2023.\\n[31] Canwen Xu and Julian McAuley. A survey on model compression and acceleration for\\npretrained language models. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nvolume 37, pages 10566–10575, 2023.\\n[32] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin\\nLin, Deng Cai, and Xiaofei He. Model compression and efficient inference for large language\\nmodels: A survey. arXiv preprint arXiv:2402.09748, 2024.\\n[33] Seungcheol Park, Jaehyeon Choi, Sojin Lee, and U Kang. A comprehensive survey of\\ncompression algorithms for language models. arXiv preprint arXiv:2401.15347, 2024.\\n[34] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,\\nXiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language\\nmodels. ACM Transactions on Intelligent Systems and Technology, 2023.\\n[35] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li,\\nBojian Xiong, Deyi Xiong, et al. Evaluating large language models: A comprehensive survey.\\narXiv preprint arXiv:2310.19736, 2023.\\n[36] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang,\\nBing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and\\nbeyond. arXiv preprint arXiv:2304.13712, 2023.\\n30'), Document(metadata={'source': 'survey.pdf', 'page': 30, 'page_label': '31'}, page_content='[37] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng,\\nZhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: A survey.\\narXiv preprint arXiv:2308.07107, 2023.\\n[38] Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng\\nZheng, and Enhong Chen. Large language models for generative information extraction: A\\nsurvey. arXiv preprint arXiv:2312.17617, 2023.\\n[39] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo,\\nand Jie M Zhang. Large language models for software engineering: Survey and open problems.\\narXiv preprint arXiv:2310.03533, 2023.\\n[40] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. Software\\ntesting with large language models: Survey, landscape, and vision. IEEE Transactions on\\nSoftware Engineering, 2024.\\n[41] Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and\\nJiachi Chen. A survey of large language models for code: Evolution, benchmarking, and future\\ntrends. arXiv preprint arXiv:2311.10372, 2023.\\n[42] Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip S Yu. Large language\\nmodels for robotics: A survey. arXiv preprint arXiv:2311.07226, 2023.\\n[43] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu\\nLu, Zichong Yang, Kuei-Da Liao, et al. A survey on multimodal large language models for\\nautonomous driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision, pages 958–979, 2024.\\n[44] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. A survey of large language models\\nfor autonomous driving. arXiv preprint arXiv:2311.01043, 2023.\\n[45] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A\\nsurvey of large language models for healthcare: from data, technology, and applications to\\naccountability and ethics. arXiv preprint arXiv:2310.05694, 2023.\\n[46] Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Sam S Chen, Peilin Zhou, Junling Liu,\\nYining Hua, Chengfeng Mao, Xian Wu, et al. A survey of large language models in medicine:\\nProgress, application, and challenge. arXiv preprint arXiv:2311.05112, 2023.\\n[47] Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, and Jie Fu.\\nPre-trained language models in biomedical domain: A systematic survey. ACM Computing\\nSurveys, 56(3):1–52, 2023.\\n[48] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. Large language models in finance: A\\nsurvey. In Proceedings of the Fourth ACM International Conference on AI in Finance, pages\\n374–382, 2023.\\n[49] Tianyu He, Guanghui Fu, Yijing Yu, Fan Wang, Jianqiang Li, Qing Zhao, Changwei Song,\\nHongzhi Qi, Dan Luo, Huijing Zou, et al. Towards a psychological generalist ai: A sur-\\nvey of current applications of large language models and future prospects. arXiv preprint\\narXiv:2312.04578, 2023.\\n[50] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. Large language models for generative\\nrecommendation: A survey and visionary discussions. arXiv preprint arXiv:2309.01157, 2023.\\n[51] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu,\\nHuifeng Guo, Yong Yu, Ruiming Tang, et al. How can recommender systems benefit from\\nlarge language models: A survey. arXiv preprint arXiv:2306.05817, 2023.\\n[52] Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat-Seng Chua. Generative recommen-\\ndation: Towards next-generation recommender paradigm. arXiv preprint arXiv:2304.03516,\\n2023.\\n31'), Document(metadata={'source': 'survey.pdf', 'page': 31, 'page_label': '32'}, page_content='[53] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo\\nZhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean: a survey on hallucination in\\nlarge language models. arXiv preprint arXiv:2309.01219, 2023.\\n[54] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qian-\\nglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in\\nlarge language models: Principles, taxonomy, challenges, and open questions. arXiv preprint\\narXiv:2311.05232, 2023.\\n[55] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation\\nmodels. arXiv preprint arXiv:2309.05922, 2023.\\n[56] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. Cognitive mirage: A review\\nof hallucinations in large language models. arXiv preprint arXiv:2309.06794, 2023.\\n[57] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.\\nACM Computing Surveys, 55(12):1–38, 2023.\\n[58] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava\\nDas. A comprehensive survey of hallucination mitigation techniques in large language models.\\narXiv preprint arXiv:2401.01313, 2024.\\n[59] Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu, Yuanzhuo Wang, and Jian Guo. A\\nsurvey on large language model hallucination via a creativity perspective. arXiv preprint\\narXiv:2402.06647, 2024.\\n[60] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck\\nDernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large\\nlanguage models: A survey. arXiv preprint arXiv:2309.00770, 2023.\\n[61] Hadas Kotek, Rikker Dockum, and David Sun. Gender bias and stereotypes in large language\\nmodels. In Proceedings of The ACM Collective Intelligence Conference, pages 12–24, 2023.\\n[62] Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large\\nlanguage models. arXiv preprint arXiv:2308.10149, 2023.\\n[63] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang\\nWang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey.\\nACM Transactions on Intelligent Systems and Technology, 2023.\\n[64] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, and Yue Zhang. A survey on large\\nlanguage model (llm) security and privacy: The good, the bad, and the ugly. arXiv preprint\\narXiv:2312.02003, 1, 2023.\\n[65] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-\\nGhazaleh. Survey of vulnerabilities in large language models revealed by adversarial attacks.\\narXiv preprint arXiv:2310.10844, 2023.\\n[66] Seth Neel and Peter Chang. Privacy issues in large language models: A survey. arXiv preprint\\narXiv:2312.06717, 2023.\\n[67] Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, and Adrian Weller. Identifying\\nand mitigating privacy risks stemming from language models: A survey. arXiv preprint\\narXiv:2310.01424, 2023.\\n[68] Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, and Yu Qiao. Attacks, defenses and\\nevaluations for llm conversation safety: A survey. arXiv preprint arXiv:2402.09283, 2024.\\n[69] Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. Security and privacy challenges of\\nlarge language models: A survey. arXiv preprint arXiv:2402.00888, 2024.\\n[70] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian\\nMin, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models.\\narXiv preprint arXiv:2303.18223, 2023.\\n32'), Document(metadata={'source': 'survey.pdf', 'page': 32, 'page_label': '33'}, page_content='[71] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar,\\nMuhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. A survey on\\nlarge language models: Applications, challenges, limitations, and practical usage. Authorea\\nPreprints, 2023.\\n[72] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar\\nSainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language\\nprocessing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):\\n1–40, 2023.\\n[73] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru,\\nRoberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al.\\nAugmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.\\n[74] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng\\nWang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey.\\narXiv preprint arXiv:2402.02716, 2024.\\n[75] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf\\nWiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress\\nand challenges. arXiv preprint arXiv:2402.01680, 2024.\\n[76] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu,\\nWenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the\\ncapability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024.\\n[77] Pengyu Zhao, Zijian Jin, and Ning Cheng. An in-depth survey of large language model-based\\nartificial intelligence agents. arXiv preprint arXiv:2309.14365, 2023.\\n[78] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li,\\nZihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et al. Exploring large language model based\\nintelligent agents: Definitions, methods, and prospects. arXiv preprint arXiv:2401.03428,\\n2024.\\n[79] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan\\nTaori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. Agent ai: Surveying the horizons\\nof multimodal interaction. arXiv preprint arXiv:2401.03568, 2024.\\n[80] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng Zhang. Llm\\nas os (llmao), agents as apps: Envisioning aios, agents and the aios-agent ecosystem. arXiv\\npreprint arXiv:2312.03815, 2023.\\n[81] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding\\nHu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A comprehensive study of\\nparameter efficient methods for pre-trained language models.arXiv preprint arXiv:2203.06904,\\n2022.\\n[82] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel:\\nLlm agents are experiential learners. arXiv preprint arXiv:2308.10144, 2023.\\n[83] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang,\\nand Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In\\nProceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,\\npages 1–22, 2023.\\n[84] Robert L Solso and Jerome Kagan. Cognitive psychology. Houghton Mifflin Harcourt P, 1979.\\n[85] Fergus IM Craik and Robert S Lockhart. Levels of processing: A framework for memory\\nresearch. Journal of verbal learning and verbal behavior, 11(6):671–684, 1972.\\n[86] Selma Leydesdorff. Memory cultures: Memory, subjectivity and recognition. Routledge, 2017.\\n[87] Philip Nicholas Johnson-Laird. Mental models: Towards a cognitive science of language,\\ninference, and consciousness. Number 6. Harvard University Press, 1983.\\n33'), Document(metadata={'source': 'survey.pdf', 'page': 33, 'page_label': '34'}, page_content='[88] John E Laird. The Soar cognitive architecture. MIT press, 2019.\\n[89] Ron Sun. Duality of the mind: A bottom-up approach toward cognition. Psychology Press,\\n2001.\\n[90] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\\n2018.\\n[91] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar\\nprompting with memory for computer control. In NeurIPS 2023 Foundation Models for\\nDecision Making Workshop, 2023.\\n[92] Ali Montazeralghaem, Hamed Zamani, and James Allan. A reinforcement learning framework\\nfor relevance feedback. In Proceedings of the 43rd international acm sigir conference on\\nresearch and development in information retrieval, pages 59–68, 2020.\\n[93] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin\\nLi, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for\\nopen-world enviroments via large language models with text-based knowledge and memory.\\narXiv preprint arXiv:2305.17144, 2023.\\n[94] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yun-\\nsheng Wu. Memochat: Tuning llms to use memos for consistent long-range open-domain\\nconversation. arXiv preprint arXiv:2308.08239, 2023.\\n[95] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen,\\nYankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong\\nWen. When large language model based agent meets user behavior analysis: A novel user\\nsimulation paradigm, 2023.\\n[96] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb:\\nAugmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901,\\n2023.\\n[97] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.\\nThink-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv\\npreprint arXiv:2311.08719, 2023.\\n[98] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and\\nZhoujun Li. Unleashing infinite-length input capacity for large-scale language models with\\nself-controlled memory system. arXiv preprint arXiv:2304.13343, 2023.\\n[99] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi\\nFan, and Anima Anandkumar. V oyager: An open-ended embodied agent with large language\\nmodels. arXiv preprint arXiv:2305.16291, 2023.\\n[100] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E\\nGonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560,\\n2023.\\n[101] Gibbeum Lee, V olker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook\\nLee. Prompted llms as chatbot modules for long open-domain conversation. arXiv preprint\\narXiv:2305.04533, 2023.\\n[102] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan,\\nXiaojiang Huang, Yanbin Lu, and Yingzhen Yang. Recmind: Large language model powered\\nagent for recommendation. arXiv preprint arXiv:2308.14296, 2023.\\n[103] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh\\nMurthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large\\nlanguage agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023.\\n[104] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and\\nYuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint\\narXiv:2210.03629, 2022.\\n34'), Document(metadata={'source': 'survey.pdf', 'page': 34, 'page_label': '35'}, page_content='[105] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for\\nrole-playing. arXiv preprint arXiv:2310.10158, 2023.\\n[106] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard\\nSchwarz. Online adaptation of language models with a memory of amortized contexts. arXiv\\npreprint arXiv:2403.04317, 2024.\\n[107] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Hu-\\natuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975,\\n2023.\\n[108] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender\\nai agent: Integrating large language models for interactive recommendations. arXiv preprint\\narXiv:2308.16505, 2023.\\n[109] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human\\nbehaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv\\npreprint arXiv:2310.06500, 2023.\\n[110] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei\\nShi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large\\nlanguage model-based ai agents. arXiv preprint arXiv:2308.03427, 2023.\\n[111] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing\\nDu, Xiaoru Hu, Hangyu Mao, Ziyue Li, et al. Tptu-v2: Boosting task planning and tool usage\\nof large language model-based agents in real-world systems. arXiv preprint arXiv:2311.11315,\\n2023.\\n[112] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili\\nWang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.\\n[113] Yi Yang, Yixuan Tang, and Kar Yan Tam. Investlm: A large language model for investment\\nusing financial domain instruction tuning. arXiv preprint arXiv:2309.13064, 2023.\\n[114] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation\\nwith tool-integrated agent systems for real-world repo-level coding challenges. arXiv preprint\\narXiv:2401.07339, 2024.\\n[115] Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical\\nchat model fine-tuned on llama model using medical domain knowledge. arXiv preprint\\narXiv:2303.14070, 2023.\\n[116] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\\nXuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise?\\nIn NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.\\n[117] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory\\nsandbox: Transparent and interactive memory management for conversational agents. In\\nAdjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and\\nTechnology, pages 1–3, 2023.\\n[118] Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and\\nSiddartha Naidu. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint\\narXiv:2308.10882, 2023.\\n[119] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski,\\nand Piotr Miło´s. Focused transformer: Contrastive training for context scaling, 2023.\\n[120] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\\nand Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\\narXiv:2307.03172, 2023.\\n[121] Peter J Denning. The locality principle. Communications of the ACM, 48(7):19–24, 2005.\\n35'), Document(metadata={'source': 'survey.pdf', 'page': 35, 'page_label': '36'}, page_content='[122] Hermann Ebbinghaus. Memory: A contribution to experimental psychology, trans. HA Ruger\\n& CE Bussenius. Teachers College.[rWvH], 1885.\\n[123] Jaap MJ Murre and Joeri Dros. Replication and analysis of ebbinghaus’ forgetting curve. PloS\\none, 10(7):e0120644, 2015.\\n[124] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus.\\nIEEE Transactions on Big Data, 7(3):535–547, 2019.\\n[125] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\\n[126] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\\nXiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+\\nreal-world apis. arXiv preprint arXiv:2307.16789, 2023.\\n[127] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n[128] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu\\nChen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv\\npreprint arXiv:2309.17452, 2023.\\n[129] Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Qian Wang, and Dinggang\\nShen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint\\narXiv:2304.01097, 2023.\\n[130] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,\\nYifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model.\\narXiv preprint arXiv:2210.02414, 2022.\\n[131] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\\npreprint arXiv:2106.09685, 2021.\\n[132] Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng\\nShu, Cheng Chen, Sekeun Kim, et al. Radiology-gpt: A large language model for radiology.\\narXiv preprint arXiv:2306.08666, 2023.\\n[133] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models.\\narXiv preprint arXiv:2104.08164, 2021.\\n[134] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast\\nmodel editing at scale. arXiv preprint arXiv:2110.11309, 2021.\\n[135] Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang,\\nPengjun Xie, Fei Huang, and Huajun Chen. Editing personality for large language models.\\n2023.\\n[136] Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, and Zhen-Hua Ling. Neighboring perturbations of\\nknowledge editing on large language models. arXiv preprint arXiv:2401.17623, 2024.\\n[137] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen\\nZhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via\\nknowledge editing. arXiv preprint arXiv:2403.14472, 2024.\\n[138] Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot\\nbenchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.\\n[139] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao\\nDu, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for\\nlong context understanding. arXiv preprint arXiv:2308.14508, 2023.\\n36'), Document(metadata={'source': 'survey.pdf', 'page': 36, 'page_label': '37'}, page_content='[140] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\\nXuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise?\\nIn NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.\\n[141] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and\\nMatthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive\\nlearning. arXiv preprint arXiv:2010.03768, 2020.\\n[142] YunDa Tsai, Mingjie Liu, and Haoxing Ren. Rtlfixer: Automatically fixing rtl syntax errors\\nwith large language models. arXiv preprint arXiv:2311.16543, 2023.\\n[143] Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang\\nFeng, Song Yan, HaoSheng Wang, et al. Chatharuhi: Reviving anime character in reality via\\nlarge language model. arXiv preprint arXiv:2308.09597, 2023.\\n[144] Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. Gamegpt: Multi-\\nagent collaborative framework for game development. arXiv preprint arXiv:2310.08067 ,\\n2023.\\n[145] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan\\nWu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking,\\neliciting, and enhancing role-playing abilities of large language models. arXiv preprint\\narXiv:2310.00746, 2023.\\n[146] Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, and Lin Gui.\\nNarrativeplay: Interactive narrative understanding. arXiv preprint arXiv:2310.01459, 2023.\\n[147] Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Lib-\\niao Peng, Jiaming Yang, Xiyao Xiao, et al. Characterglm: Customizing chinese conversational\\nai characters with large language models. arXiv preprint arXiv:2311.16832, 2023.\\n[148] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo,\\nGuangyu Robert Yang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time\\nsocial interactions. arXiv preprint arXiv:2310.02172, 2023.\\n[149] Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian McAuley, Wayne Xin Zhao, Leyu\\nLin, and Ji-Rong Wen. Agentcf: Collaborative learning with autonomous language agents for\\nrecommender systems. arXiv preprint arXiv:2310.09233, 2023.\\n[150] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill,\\nand Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent\\nsimulation of world wars. arXiv preprint arXiv:2311.17227, 2023.\\n[151] Haochun Wang, Sendong Zhao, Zewen Qiang, Zijian Li, Nuwa Xi, Yanrui Du, MuZhen Cai,\\nHaoqiang Guo, Yuhan Chen, Haoming Xu, et al. Knowledge-tuning large language models\\nwith structured medical knowledge bases for reliable response generation in chinese. arXiv\\npreprint arXiv:2309.04175, 2023.\\n[152] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho,\\nCarl Yang, and May D Wang. Ehragent: Code empowers large language models for complex\\ntabular reasoning on electronic health records. arXiv preprint arXiv:2401.07128, 2024.\\n[153] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\\nmulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\\n[154] Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. Tradinggpt: Multi-\\nagent system with layered memory and distinct characters for enhanced financial trading\\nperformance. arXiv preprint arXiv:2309.03736, 2023.\\n[155] Saizhuo Wang, Hang Yuan, Lionel M Ni, and Jian Guo. Quantagent: Seeking holy grail in\\ntrading by self-improving large language model. arXiv preprint arXiv:2402.03755, 2024.\\n37'), Document(metadata={'source': 'survey.pdf', 'page': 37, 'page_label': '38'}, page_content='[156] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu,\\nJordan W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced llm trading\\nagent with layered memory and character design. arXiv e-prints, pages arXiv–2311, 2023.\\n[157] Kelvin JL Koa, Yunshan Ma, Ritchie Ng, and Tat-Seng Chua. Learning to generate explainable\\nstock predictions using self-reflective large language models.arXiv preprint arXiv:2402.03659,\\n2024.\\n[158] Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, Jiezhong\\nQiu, Jianzhang Pan, Yi Huang, Qun Fang, Pheng Ann Heng, and Guangyong Chen. Chemist-x:\\nLarge language model-empowered agent for reaction condition recommendation in chemical\\nsynthesis, 2024.\\n[159] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,\\nZhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents\\nwith memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997,\\n2023.\\n[160] Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu,\\nShuai Fan, Guodong Shen, et al. Chemdfm: Dialogue foundation model for chemistry. arXiv\\npreprint arXiv:2401.14818, 2024.\\n[161] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and Ji Yan. Larp: Language-agent\\nrole play for open-world games. arXiv preprint arXiv:2312.17653, 2023.\\n[162] Zi-Yi Chen, Fan-Kai Xie, Meng Wan, Yang Yuan, Miao Liu, Zong-Guo Wang, Sheng Meng,\\nand Yan-Gang Wang. Matchat: A large language model and application service platform for\\nmaterials science. Chinese Physics B, 32(11):118104, 2023.\\n[163] Nian Li, Chen Gao, Yong Li, and Qingmin Liao. Large language model-empowered agents for\\nsimulating macroeconomic activities. arXiv preprint arXiv:2310.10436, 2023.\\n[164] Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, and\\nBing Qin. Kwaiagents: Generalized information-seeking agent system with large language\\nmodels. arXiv preprint arXiv:2312.04889, 2023.\\n[165] Odma Byambasuren, Yunfei Yang, Zhifang Sui, Damai Dai, Baobao Chang, Sujian Li, and\\nHongying Zan. Preliminary study on the construction of chinese medical knowledge graph.\\nJournal of Chinese Information Processing, 33(10):1–9, 2019.\\n[166] Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, and\\nQingsong Wen. Rcagent: Cloud root cause analysis by autonomous agents with tool-augmented\\nlarge language models. arXiv preprint arXiv:2310.16340, 2023.\\n[167] Zhangcheng Qiang, Weiqing Wang, and Kerry Taylor. Agent-om: Leveraging large language\\nmodels for ontology matching. arXiv preprint arXiv:2312.00326, 2023.\\n[168] Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi,\\nLiang He, and Yu Qiao. Dilu: A knowledge-driven approach to autonomous driving with large\\nlanguage models. arXiv preprint arXiv:2309.16292, 2023.\\n[169] Zhitao Wang, Wei Wang, Zirao Li, Long Wang, Can Yi, Xinjie Xu, Luyang Cao, Hanjing Su,\\nShouzhi Chen, and Jun Zhou. Xuat-copilot: Multi-agent collaborative system for automated\\nuser acceptance testing with large language model. arXiv preprint arXiv:2401.02705, 2024.\\n[170] Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and Chuchu Fan. Scalable multi-\\nrobot collaboration with large language models: Centralized or decentralized systems? arXiv\\npreprint arXiv:2309.15943, 2023.\\n[171] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with\\nlarge language models. arXiv preprint arXiv:2307.04738, 2023.\\n[172] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. From text to tactic: Evaluating llms\\nplaying the game of avalon. arXiv preprint arXiv:2310.05036, 2023.\\n38'), Document(metadata={'source': 'survey.pdf', 'page': 38, 'page_label': '39'}, page_content='[173] Bing Liu. Lifelong machine learning: a paradigm for continuous learning. Frontiers of\\nComputer Science, 11:359–361, 2017.\\n[174] Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to\\nsimulate multiple humans and replicate human subject studies. In International Conference on\\nMachine Learning, pages 337–371. PMLR, 2023.\\n39')]\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"survey.pdf\")\n",
    "pages = loader.load()\n",
    "print(\"First 1000 Characters: \", pages[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\n",
    "for i in range(len(pages)):\n",
    "    document += pages[i].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(chunks, use_tokens=False):\n",
    "    # Print the chunks of interest\n",
    "    print(\"\\nNumber of Chunks:\", len(chunks))\n",
    "    print(\"\\n\", \"=\"*50, \"40th Chunk\", \"=\"*50,\"\\n\", chunks[40])\n",
    "    print(\"\\n\", \"=\"*50, \"41st Chunk\", \"=\"*50,\"\\n\", chunks[41])\n",
    "    \n",
    "    chunk1, chunk2 = chunks[20], chunks[21]\n",
    "    \n",
    "    if use_tokens:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        tokens1 = encoding.encode(chunk1)\n",
    "        tokens2 = encoding.encode(chunk2)\n",
    "        \n",
    "        # Find overlapping tokens\n",
    "        for i in range(len(tokens1), 0, -1):\n",
    "            if tokens1[-i:] == tokens2[:i]:\n",
    "                overlap = encoding.decode(tokens1[-i:])\n",
    "                print(\"\\n\", \"=\"*50, f\"\\nOverlapping text ({i} tokens):\", overlap)\n",
    "                return\n",
    "        print(\"\\nNo token overlap found\")\n",
    "    else:\n",
    "        # Find overlapping characters\n",
    "        for i in range(min(len(chunk1), len(chunk2)), 0, -1):\n",
    "            if chunk1[-i:] == chunk2[:i]:\n",
    "                print(\"\\n\", \"=\"*50, f\"\\nOverlapping text ({i} chars):\", chunk1[-i:])\n",
    "                return\n",
    "        print(\"\\nNo character overlap found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Text Splitting\n",
    "\n",
    "The simplest form of chunking would be simply counting some number of characters and splitting at that count.\n",
    "\n",
    "![Example Image](.\\images\\fixed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(document, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    stride = chunk_size - overlap\n",
    "    current_idx = 0\n",
    "    \n",
    "    while current_idx < len(document):\n",
    "        # Take chunk_size characters starting from current_idx\n",
    "        chunk = document[current_idx:current_idx + chunk_size]\n",
    "        if not chunk:  # Break if we're out of text\n",
    "            break\n",
    "        chunks.append(chunk)\n",
    "        current_idx += stride  # Move forward by stride\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Chunks: 368\n",
      "\n",
      " ================================================== 40th Chunk ================================================== \n",
      " y aim to summarize techniques that can\n",
      "be leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\n",
      "comprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\n",
      "training LLMs. Shen et al. [9], Wang et al.[10] and Liu et al. [11] present surveys on the alignment of\n",
      "LLMs, which is a key requirement for LLMs to produce outputs con\n",
      "\n",
      " ================================================== 41st Chunk ================================================== \n",
      " sistent with human values. Gao\n",
      "et al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\n",
      "is key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\n",
      "et al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\n",
      "is fundamental for LLMs to expand their capability in domains that requi\n",
      "\n",
      " ================================================== \n",
      "Overlapping text (20 chars): . . . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "character_chunks = chunk_text(document, chunk_size=400, overlap=0)\n",
    "\n",
    "analyze_chunks(character_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Chunks: 368\n",
      "\n",
      " ================================================== 40th Chunk ================================================== \n",
      " y aim to summarize techniques that can\n",
      "be leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\n",
      "comprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\n",
      "training LLMs. Shen et al. [9], Wang et al.[10] and Liu et al. [11] present surveys on the alignment of\n",
      "LLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\n",
      "et al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\n",
      "is key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\n",
      "et al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\n",
      "is fundamental for LLMs to expand their capability in domains that requi\n",
      "\n",
      " ================================================== 41st Chunk ================================================== \n",
      " sistent with human values. Gao\n",
      "et al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\n",
      "is key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\n",
      "et al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\n",
      "is fundamental for LLMs to expand their capability in domains that require specialized knowledge.\n",
      "Wang et al. [13], Yao et al. [14], Wang et al. [15], Feng et al. [16] and Zhang et al. [17] present\n",
      "surveys on the direction of LLM knowledge editing, which is important for customizing LLMs to\n",
      "satisfy specific requirements. Huang et al. [19], Wang et al. [20] and Pawar et al. [21] focus on\n",
      "long-context capabilities of LLMs, which is critical for LLMs to process more info\n",
      "\n",
      " ================================================== \n",
      "Overlapping text (400 chars): . . . . . . . . . . 22\n",
      "6.2.1 Conversation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "6.2.2 Multi-source Question-answering . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "6.2.3 Long-context Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "26.2.4 Other Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "6.3 Discussions . . . . . .\n"
     ]
    }
   ],
   "source": [
    "#Chunk size of 800 Characters, 400 overlap\n",
    "character_chunks = chunk_text(document, chunk_size=800, overlap=400)\n",
    "analyze_chunks(character_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Text Splitting\n",
    "\n",
    "But language models (the end users of chunked text usually) don't operate at the character level. Instead they use tokens, or common sequences of characters that represent frequent words, word pieces, and subwords.\n",
    "\n",
    "This means character-based splitting isn't ideal because:\n",
    "\n",
    "1. A 500-character chunk might contain anywhere from 100-500 tokens depending on the text\n",
    "2. Different languages and character sets encode to very different numbers of tokens\n",
    "3. We might hit token limits in our LLM without realizing it\n",
    "\n",
    "\n",
    "Tokenizers like 'cl100k_base' implement Byte-Pair Encoding (BPE) - a compression algorithm that creates a vocabulary by iteratively merging the most frequent pairs of bytes or characters. The '100k' refers to its vocab size, determining the balance between compression and representation granularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"Count tokens in a text string using tiktoken\"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return print(f\"Number of tokens: {len(encoder.encode(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Chunks: 92\n",
      "\n",
      " ================================================== 40th Chunk ================================================== \n",
      "  in a structured database. When writing into the database, similar contents will\n",
      "be stored in the same group. In SCM [98], it designs a memory controller to decide when to execute\n",
      "the operations. The controller serves as a guide for the whole memory module. In MemGPT [100],\n",
      "the memory writing is entirely self-directed. The agents can autonomously update the memory based\n",
      "on the contexts. In MemoChat [94], the agents summarize each conversation segment by abstracting\n",
      "the mainly discussed topics and storing them as keys for indexing memory pieces.\n",
      "Discussion. Previous research indicates that designing the strategy of information extraction during\n",
      "the memory writing operation is vital [94]. This is because the original information is commonly\n",
      "lengthy and noisy. Besides, different environments may provide various forms of feedback, and how\n",
      "to extract and represent the information as memory is also significant for memory writing.\n",
      "5.3.2 Memory Management\n",
      "For human beings, memory information is constantly processed and abstracted in the brains. The\n",
      "memory in the agent can also be managed by reflecting to generate higher-level memories, merging\n",
      "redundant memory entries, and forgetting unimportant, early memories.\n",
      "Representative Studies. In MemoryBank [6], the agents process and distill the conversations into a\n",
      "high-level summary of daily events, similar to how humans recall key aspects of their experiences.\n",
      "Through long-term interactions, they continually evaluate and refine their knowledge, generating daily\n",
      "insights into personality traits. In V oyager [99], the agents are able to refine their memory based on\n",
      "the feedback of the environment. In Generative Agents [83], the agents can reflect to get higher-level\n",
      "information, where the abstract thoughts are generated from agents. The reflection process will be\n",
      "activated when there are accumulated events that are enough to address. For GITM [93], in order to\n",
      "establish common reference plans for various situations, key actions from multiple plans are further\n",
      "summarized in the memory module\n",
      "\n",
      " ================================================== 41st Chunk ================================================== \n",
      " .\n",
      "Discussion. Most of the memory management operations are inspired by the working mechanism of\n",
      "human brains. With the strong capability of LLMs to simulate human minds, these operations can\n",
      "help the agents to better generate high-level information and interact with environments.\n",
      "18Table 3: Summarization of the memory operations. If a model does not have special designs on the\n",
      "memory operations, we use ◦ to label it, otherwise, it is denoted by ✓. × means that the memory\n",
      "operations are not discussed in the paper.\n",
      "Models Writing\n",
      "Management\n",
      "ReadingMerging Reflection Forgetting\n",
      "MemoryBank [6] ✓ ✓ ✓ ✓ ✓\n",
      "RET-LLM [7] ✓ × × × ✓\n",
      "ChatDB [96] ✓ × ✓ × ✓\n",
      "TiM [97] ✓ ✓ × ✓ ✓\n",
      "SCM [98] ✓ ✓ × × ✓\n",
      "V oyager [99] ✓ × ✓ × ✓\n",
      "MemGPT [100] ✓ × ✓ × ✓\n",
      "MemoChat [94] ✓ × × × ✓\n",
      "MPC [101] ✓ × × × ✓\n",
      "Generative Agents [83] ✓ × ✓ ✓ ✓\n",
      "RecMind [102] ◦ × × × ✓\n",
      "Retroformer [103] ✓ ✓ ✓ × ◦\n",
      "ExpeL [82] ✓ ✓ ✓ × ◦\n",
      "Synapse [91] ✓ × × × ✓\n",
      "GITM [93] ◦ ✓ ✓ × ✓\n",
      "ReAct [104] ◦ × × × ◦\n",
      "Reflexion [5] ✓ ✓ ✓ × ◦\n",
      "RecAgent [95] ✓ ✓ ✓ ✓ ✓\n",
      "Character-LLM [105] ✓ × × × ◦\n",
      "MAC [106] ✓ ✓ ✓ × ✓\n",
      "Huatuo [107] ✓ × × × ◦\n",
      "ChatDev [1] ✓ × ✓ × ✓\n",
      "InteRecAgent [108] ✓ ✓ ✓ × ✓\n",
      "MetaAgents [109]\n",
      "\n",
      "No token overlap found\n"
     ]
    }
   ],
   "source": [
    "fixed_token_chunker = FixedTokenChunker(\n",
    "    chunk_size=400, \n",
    "    chunk_overlap=0,\n",
    "    encoding_name=\"cl100k_base\"\n",
    ")\n",
    "\n",
    "token_chunks = fixed_token_chunker.split_text(document)\n",
    "\n",
    "analyze_chunks(token_chunks, use_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Chunks: 183\n",
      "\n",
      " ================================================== 40th Chunk ================================================== \n",
      "  and arrival time in [step 1] and\n",
      "[step 2]. For task (B), the agent has to choose a movie for Alice at [step 3]; at this time, its memory\n",
      "contains the arranged time to watch films.\n",
      "3.3 Broad Definition of the Agent Memory\n",
      "In a broad sense, the memory of the agent can come from much wider sources, for example,\n",
      "the information across different trials and the external knowledge beyond the agent-environment\n",
      "interactions. Formally, given a series of sequential tasks {T1, T2, ...,TK}, for task Tk, the memory\n",
      "information at step t comes from three sources: (1) the historical information within the same\n",
      "trial, that is, ξk\n",
      "t = {ak\n",
      "1 , ok\n",
      "1 , ..., ak\n",
      "t−1, ok\n",
      "t−1}, where we add superscript k to label the task index.\n",
      "(2) The historical information across different trials, that is, Ξk = {ξ1, ξ2, ..., ξk−1, ξk′\n",
      "}, where\n",
      "ξj (j ∈ {1, ..., k− 1}) represents the trials of task j1, and ξk′\n",
      "denotes the previously explored trials\n",
      "for task Tk. (3) External knowledge, which is represented by Dk\n",
      "t . The memory of the agent is derived\n",
      "based on (ξk\n",
      "t , Ξk, Dk\n",
      "t ). In the above toy example, for task (A), if there are several failed trials,\n",
      "that is, the feedback from Alice is negative, then these trials can be incorporated into the agent’s\n",
      "memory to avoid future similar errors (corresponding to ξk′\n",
      "). In addition, for task (B), the agent\n",
      "may recommend movies relevant to the attractions that Alice has visited in task (A) to capture her\n",
      "recent preferences\n",
      "\n",
      " ================================================== 41st Chunk ================================================== \n",
      "  is, Ξk = {ξ1, ξ2, ..., ξk−1, ξk′\n",
      "}, where\n",
      "ξj (j ∈ {1, ..., k− 1}) represents the trials of task j1, and ξk′\n",
      "denotes the previously explored trials\n",
      "for task Tk. (3) External knowledge, which is represented by Dk\n",
      "t . The memory of the agent is derived\n",
      "based on (ξk\n",
      "t , Ξk, Dk\n",
      "t ). In the above toy example, for task (A), if there are several failed trials,\n",
      "that is, the feedback from Alice is negative, then these trials can be incorporated into the agent’s\n",
      "memory to avoid future similar errors (corresponding to ξk′\n",
      "). In addition, for task (B), the agent\n",
      "may recommend movies relevant to the attractions that Alice has visited in task (A) to capture her\n",
      "recent preferences (corresponding to {ξ1, ξ2, ..., ξk−1}). In the agent decision process, it has also\n",
      "referred to the magazine Attractions in Beijing for making trip plans, which is the external knowledge\n",
      "(corresponding to Dk\n",
      "t ) for the current task Tk.\n",
      "3.4 Memory-assisted Agent-Environment Interaction\n",
      "As mentioned at the beginning of Section 3, there are three key phases in the agent-environment\n",
      "interaction process. The agent memory module implements these phases through three operations\n",
      "including memory writing, memory management, and memory reading.\n",
      "Memory Writing. This operation aims to project the raw observations into the actually stored\n",
      "memory contents, which are more informative [7] and concise [6]. It corresponds to the first phase of\n",
      "the agent-environment interaction process. Given a task Tk, if the agent takes an action ak\n",
      "t at step t,\n",
      "and the environment provides an observation ok\n",
      "t , then the memory\n",
      "\n",
      " ================================================== \n",
      "Overlapping text (200 tokens):  et al. [70] present the first comprehensive survey to summarize the\n",
      "background, evolution paths, model architectures, training methodologies, and evaluation strategies\n",
      "of LLMs. Hadi et al. [71] and Min et al. [72] also conduct LLM surveys from the holistic view,\n",
      "which, however, provide different taxonomies and understandings on LLMs. Following these surveys,\n",
      "people dive into specific aspects of LLMs and review the corresponding milestone studies and key\n",
      "technologies. These aspects can be classified into four categories including the fundamental problems,\n",
      "evaluation, applications, and challenges of LLMs.\n",
      "Fundamental problems. The surveys in this category aim to summarize techniques that can\n",
      "be leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\n",
      "comprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\n",
      "training LLMs. Shen et al. [9], Wang et\n"
     ]
    }
   ],
   "source": [
    "fixed_token_chunker = FixedTokenChunker(\n",
    "    chunk_size=400, \n",
    "    chunk_overlap=200,\n",
    "    encoding_name=\"cl100k_base\"\n",
    ")\n",
    "\n",
    "token_overlap_chunks = fixed_token_chunker.split_text(document)\n",
    "\n",
    "analyze_chunks(token_overlap_chunks, use_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Character Text Splitter\n",
    "\n",
    "But simply counting tokens or characters can only get us so much. When we write, we naturally separate text into paragraphs, sentences, and other logical units. The recursive character text splitter tries to intelligently split text by looking for natural separators in order, while respecting a maximum character length.\n",
    "\n",
    "First, it makes a complete pass over the entire document using paragraph breaks (\\n\\n), creating an initial set of chunks. Then for any chunks that exceed the size limit, it recursively processes them using progressively smaller separators:\n",
    "\n",
    "1. First tries to split on paragraph breaks (\\n\\n)\n",
    "2. If chunks are still too big, tries line breaks (\\n)\n",
    "3. Then sentence boundaries (., ?, !)\n",
    "4. Then words ( )\n",
    "5. Finally, if no other separators work, splits on individual characters (\"\")\n",
    "\n",
    "This way, the splitter preserves as much natural structure as possible - only drilling down to smaller separators when necessary to meet the size limit. A chunk that's already small enough stays intact, while larger chunks get progressively broken down until they fit.\n",
    "\n",
    "![Example Image](.\\images\\recursive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Chunks: 194\n",
      "\n",
      " ================================================== 40th Chunk ================================================== \n",
      " its memory contains the information about the selected attractions and arrival time in [step 1] and\n",
      "[step 2]. For task (B), the agent has to choose a movie for Alice at [step 3]; at this time, its memory\n",
      "contains the arranged time to watch films.\n",
      "3.3 Broad Definition of the Agent Memory\n",
      "In a broad sense, the memory of the agent can come from much wider sources, for example,\n",
      "the information across different trials and the external knowledge beyond the agent-environment\n",
      "interactions. Formally, given a series of sequential tasks {T1, T2, ...,TK}, for task Tk, the memory\n",
      "information at step t comes from three sources: (1) the historical information within the same\n",
      "trial, that is, ξk\n",
      "t = {ak\n",
      "1 , ok\n",
      "1 , ..., ak\n",
      "t−1, ok\n",
      "t−1}, where we add superscript k to label the task index.\n",
      "\n",
      " ================================================== 41st Chunk ================================================== \n",
      " (2) The historical information across different trials, that is, Ξk = {ξ1, ξ2, ..., ξk−1, ξk′\n",
      "}, where\n",
      "ξj (j ∈ {1, ..., k− 1}) represents the trials of task j1, and ξk′\n",
      "denotes the previously explored trials\n",
      "for task Tk. (3) External knowledge, which is represented by Dk\n",
      "t . The memory of the agent is derived\n",
      "based on (ξk\n",
      "t , Ξk, Dk\n",
      "t ). In the above toy example, for task (A), if there are several failed trials,\n",
      "that is, the feedback from Alice is negative, then these trials can be incorporated into the agent’s\n",
      "memory to avoid future similar errors (corresponding to ξk′\n",
      "). In addition, for task (B), the agent\n",
      "may recommend movies relevant to the attractions that Alice has visited in task (A) to capture her\n",
      "\n",
      "No character overlap found\n"
     ]
    }
   ],
   "source": [
    "recursive_character_chunker = RecursiveTokenChunker(\n",
    "    chunk_size=800,  # Character Length\n",
    "    chunk_overlap=0,  # Overlap\n",
    "    length_function=len,  # Character length with len()\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"] # According to Research\n",
    ")\n",
    "\n",
    "recursive_character_chunks = recursive_character_chunker.split_text(document)\n",
    "analyze_chunks(recursive_character_chunks, use_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Chunks: 373\n",
      "\n",
      " ================================================== 40th Chunk ================================================== \n",
      " Fundamental problems. The surveys in this category aim to summarize techniques that can\n",
      "be leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\n",
      "comprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\n",
      "training LLMs. Shen et al. [9], Wang et al.[10] and Liu et al. [11] present surveys on the alignment of\n",
      "LLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\n",
      "et al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\n",
      "is key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\n",
      "et al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\n",
      "\n",
      " ================================================== 41st Chunk ================================================== \n",
      " LLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\n",
      "et al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\n",
      "is key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\n",
      "et al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\n",
      "is fundamental for LLMs to expand their capability in domains that require specialized knowledge.\n",
      "Wang et al. [13], Yao et al. [14], Wang et al. [15], Feng et al. [16] and Zhang et al. [17] present\n",
      "surveys on the direction of LLM knowledge editing, which is important for customizing LLMs to\n",
      "satisfy specific requirements. Huang et al. [19], Wang et al. [20] and Pawar et al. [21] focus on\n",
      "\n",
      " ================================================== \n",
      "Overlapping text (349 chars): 6.2.1 Conversation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "6.2.2 Multi-source Question-answering . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "6.2.3 Long-context Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "26.2.4 Other Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n"
     ]
    }
   ],
   "source": [
    "recursive_character_chunker = RecursiveTokenChunker(\n",
    "    chunk_size=800,  # Character Length\n",
    "    chunk_overlap=400,  # Overlap\n",
    "    length_function=len,  # Character length with len()\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"] # According to Research\n",
    ")\n",
    "\n",
    "recursive_character_overlap_chunks = recursive_character_chunker.split_text(document)\n",
    "analyze_chunks(recursive_character_overlap_chunks, use_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Chunker\n",
    "\n",
    "Greg Kamradt popularized what's known as the semantic chunker with his 5 Levels of Text Splitting notebook here which takes a different approach from fixed character/token chunking. Instead of splitting text at predetermined positions or separators, it uses embeddings to find natural semantic boundaries in the text while maintaining consistent chunk sizes.\n",
    "\n",
    "Chroma modified the algorithm to provide better size control through binary search. The chunker first splits text into small fixed-size pieces (around 50 tokens) using standard recursive splitting with separators. For each piece, it looks at surrounding context (3 segments before and after) to understand the local meaning - this helps maintain semantic coherence across potential split points.\n",
    "\n",
    "After embedding these contextualized pieces, it calculates cosine distances between consecutive segments. Higher distances suggest natural topic transitions that make good splitting points. But rather than using Kamradt's original fixed percentile approach for choosing split points, Chroma's version uses binary search to find a similarity threshold that produces chunks close to the target size.\n",
    "\n",
    "The binary search starts with limits of 0.0 and 1.0, calculating the midpoint threshold and counting how many splits it would create. If there are too many splits, it raises the threshold by adjusting the lower limit; too few splits, it lowers the threshold by adjusting the upper limit. This continues until it finds a threshold that creates chunks of approximately the desired size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
