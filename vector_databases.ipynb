{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Between Vector Index and Vector Database\n",
    "\n",
    "The terms **vector index** and **vector database** are related but serve different purposes in managing and retrieving vectorized data.\n",
    "\n",
    "**1. Vector Index**\n",
    "A **vector index** is a data structure that enables efficient similarity search on high-dimensional vector data. It speeds up retrieval for nearest neighbor searches.\n",
    "\n",
    "**Key Features:**\n",
    "- Designed for fast similarity searches on high-dimensional data.\n",
    "- Uses indexing methods like:\n",
    "  - **HNSW (Hierarchical Navigable Small World graphs)** – Used in FAISS, Milvus, Weaviate.\n",
    "  - **IVF (Inverted File Index)** – Used in FAISS for partitioning vectors.\n",
    "  - **LSH (Locality-Sensitive Hashing)** – Used in ANN-based searches.\n",
    "  - **KD-Trees, Ball Trees, VP-Trees** – Used in classical nearest neighbor searches.\n",
    "- **Does not store raw vectors** but optimizes their access and retrieval.\n",
    "- Helps reduce computational cost ofbrute-force searches.\n",
    "\n",
    "**Example Usage:**\n",
    "- In FAISS, an IVF-PQ index speeds up approximate nearest neighbor searches.\n",
    "- HNSW-based indexes allow rapid semantic search in vector databases.\n",
    "\n",
    "\n",
    "\n",
    "**2. Vector Database**\n",
    "A vector database is a complete data management system designed to store, index, and query vectorized data efficiently. It includes a vector index but also provides other database functionalities.\n",
    "\n",
    "**Key Features:**\n",
    "- Stores both raw vector embeddings and metadata (e.g., text, labels, categories).\n",
    "- Uses vector indexes internally for optimized searches.\n",
    "- Provides scalability, persistence, and distributed search.\n",
    "- Supports hybrid search (vector similarity + structured queries).\n",
    "- **Examples:**\n",
    "  - **FAISS** – Optimized for in-memory searches.\n",
    "  - **Milvus** – Open-source vector database with distributed storage.\n",
    "  - **Weaviate** – Supports hybrid search with metadata filtering.\n",
    "  - **Pinecone** – Managed vector database with automatic indexing.\n",
    "  - **ChromaDB** – Lightweight, developer-friendly vector store.\n",
    "\n",
    "**Example Usage:**\n",
    "- A semantic search engine for legal documents using text embeddings.\n",
    "- Recommendation systems that find similar products via image/text embeddings.\n",
    "- Retrieval-Augmented Generation (RAG) pipelines for LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional databases work, they store strings, numbers, and other types of scalar data in rows and columns. On the other hand, a vector database operates on vectors, so the way it’s optimized and queried is quite different.\n",
    "\n",
    "In traditional databases, we are usually querying for rows in the database where the value usually exactly matches our query. In vector databases, we apply a similarity metric to find a vector that is the most similar to our query.\n",
    "\n",
    "A vector database uses a combination of different algorithms that all participate in Approximate Nearest Neighbor (ANN) search. These algorithms optimize the search through hashing, quantization, or graph-based search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Retrieval-Augmented Generation (RAG) pipeline consists of three main phases: indexing, retrieval, and generation. In the indexing phase, raw documents are processed, transformed into vector embeddings using an embedding model, and stored in a vector database for efficient retrieval. During the retrieval phase, when a user query is received, the system searches for the most relevant document embeddings using vector similarity search techniques like FAISS, Milvus, or Pinecone. Finally, in the generation phase, a large language model (LLM) uses the retrieved documents to generate a contextually accurate response, enhancing the reliability of the output by incorporating external knowledge.\n",
    "\n",
    "![Example Image](RAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sorou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 60\u001b[0m\n\u001b[0;32m     52\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     53\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retriever \u001b[38;5;241m|\u001b[39m format_docs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Question\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m display(\u001b[43mMarkdown\u001b[49m(rag_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Task Decomposition?\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "display(Markdown(rag_chain.invoke(\"What is Task Decomposition?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
